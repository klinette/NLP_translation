{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xKax1961_RY6","executionInfo":{"status":"ok","timestamp":1670697385468,"user_tz":300,"elapsed":6538,"user":{"displayName":"Linette Kunin","userId":"07994646427956674729"}},"outputId":"9d81a8be-41e5-405e-bbc4-e0700c61a10e"},"outputs":[{"output_type":"stream","name":"stderr","text":["Cloning into 'neuralcoref'...\n","Checking out files:  99% (150/151)   \rChecking out files: 100% (151/151)   \rChecking out files: 100% (151/151), done.\n"]}],"source":["%%bash\n","!(stat -t /usr/local/lib/*/dist-packages/google/colab > /dev/null 2>&1) && exit \n","git clone https://github.com/huggingface/neuralcoref.git"]},{"cell_type":"code","source":["!pip install -U spacy\n","!python -m spacy download en_core_web_sm\n","\n","%cd neuralcoref\n","\n","!pip install -r requirements.txt\n","!pip install -e ."],"metadata":{"id":"-hEBej7jDV5Y","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670697463484,"user_tz":300,"elapsed":76427,"user":{"displayName":"Linette Kunin","userId":"07994646427956674729"}},"outputId":"a141eb59-b366-4420-d875-bf1134329949"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: spacy in /usr/local/lib/python3.8/dist-packages (3.4.3)\n","Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.10.1)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.23.0)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.0.8)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.0.8)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.3.0)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (8.1.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.0.9)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy) (57.4.0)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.11.3)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.0.3)\n","Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.10.0)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.4.5)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.0.7)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.21.6)\n","Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.7.0)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.10.2)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.0.10)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (4.64.1)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (21.3)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->spacy) (3.0.9)\n","Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from pathy>=0.3.5->spacy) (5.2.1)\n","Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.4.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.9.24)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.3)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.9)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy) (7.1.2)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy) (2.0.1)\n","/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n","  warnings.warn(\"Can't initialize NVML\")\n","2022-12-10 18:36:51.845658: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting en-core-web-sm==3.4.1\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.1/en_core_web_sm-3.4.1-py3-none-any.whl (12.8 MB)\n","\u001b[K     |████████████████████████████████| 12.8 MB 3.2 MB/s \n","\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.8/dist-packages (from en-core-web-sm==3.4.1) (3.4.3)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.5)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (57.4.0)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.4.5)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.10.2)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.23.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.64.1)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.8)\n","Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.0)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.10)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.7)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.3.0)\n","Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.0)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.8)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.9)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.3)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.21.6)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (21.3)\n","Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.11.3)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.9)\n","Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (5.2.1)\n","Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2022.9.24)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.4)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.9)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.0.3)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (7.1.2)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.1)\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_sm')\n","/content/neuralcoref\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting spacy<3.0.0,>=2.1.0\n","  Downloading spacy-2.3.8-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.0 MB)\n","\u001b[K     |████████████████████████████████| 5.0 MB 5.5 MB/s \n","\u001b[?25hRequirement already satisfied: cython>=0.25 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 2)) (0.29.32)\n","Requirement already satisfied: pytest in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 3)) (3.6.4)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (57.4.0)\n","Collecting plac<1.2.0,>=0.9.6\n","  Downloading plac-1.1.3-py2.py3-none-any.whl (20 kB)\n","Collecting catalogue<1.1.0,>=0.0.7\n","  Downloading catalogue-1.0.2-py2.py3-none-any.whl (16 kB)\n","Collecting thinc<7.5.0,>=7.4.1\n","  Downloading thinc-7.4.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n","\u001b[K     |████████████████████████████████| 1.1 MB 49.6 MB/s \n","\u001b[?25hCollecting srsly<1.1.0,>=1.0.2\n","  Downloading srsly-1.0.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (211 kB)\n","\u001b[K     |████████████████████████████████| 211 kB 48.0 MB/s \n","\u001b[?25hRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (1.0.9)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (1.21.6)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (2.0.7)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (2.23.0)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (0.10.1)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (3.0.8)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (4.64.1)\n","Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (0.7.9)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (2022.9.24)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (3.0.4)\n","Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.8/dist-packages (from pytest->-r requirements.txt (line 3)) (1.4.1)\n","Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.8/dist-packages (from pytest->-r requirements.txt (line 3)) (1.11.0)\n","Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.8/dist-packages (from pytest->-r requirements.txt (line 3)) (22.1.0)\n","Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from pytest->-r requirements.txt (line 3)) (9.0.0)\n","Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.8/dist-packages (from pytest->-r requirements.txt (line 3)) (0.7.1)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.8/dist-packages (from pytest->-r requirements.txt (line 3)) (1.15.0)\n","Installing collected packages: srsly, plac, catalogue, thinc, spacy\n","  Attempting uninstall: srsly\n","    Found existing installation: srsly 2.4.5\n","    Uninstalling srsly-2.4.5:\n","      Successfully uninstalled srsly-2.4.5\n","  Attempting uninstall: catalogue\n","    Found existing installation: catalogue 2.0.8\n","    Uninstalling catalogue-2.0.8:\n","      Successfully uninstalled catalogue-2.0.8\n","  Attempting uninstall: thinc\n","    Found existing installation: thinc 8.1.5\n","    Uninstalling thinc-8.1.5:\n","      Successfully uninstalled thinc-8.1.5\n","  Attempting uninstall: spacy\n","    Found existing installation: spacy 3.4.3\n","    Uninstalling spacy-3.4.3:\n","      Successfully uninstalled spacy-3.4.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","en-core-web-sm 3.4.1 requires spacy<3.5.0,>=3.4.0, but you have spacy 2.3.8 which is incompatible.\n","confection 0.0.3 requires srsly<3.0.0,>=2.4.0, but you have srsly 1.0.6 which is incompatible.\u001b[0m\n","Successfully installed catalogue-1.0.2 plac-1.1.3 spacy-2.3.8 srsly-1.0.6 thinc-7.4.6\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Obtaining file:///content/neuralcoref\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from neuralcoref==4.0) (1.21.6)\n","Collecting boto3\n","  Downloading boto3-1.26.27-py3-none-any.whl (132 kB)\n","\u001b[K     |████████████████████████████████| 132 kB 4.9 MB/s \n","\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from neuralcoref==4.0) (2.23.0)\n","Requirement already satisfied: spacy<3.0.0,>=2.1.0 in /usr/local/lib/python3.8/dist-packages (from neuralcoref==4.0) (2.3.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref==4.0) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref==4.0) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref==4.0) (2022.9.24)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref==4.0) (1.24.3)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (2.0.7)\n","Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (0.7.9)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.8/dist-packages (from spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (1.0.2)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (1.0.6)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (4.64.1)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.8/dist-packages (from spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (1.1.3)\n","Requirement already satisfied: thinc<7.5.0,>=7.4.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (7.4.6)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (1.0.9)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (3.0.8)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (57.4.0)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (0.10.1)\n","Collecting jmespath<2.0.0,>=0.7.1\n","  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n","Collecting s3transfer<0.7.0,>=0.6.0\n","  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n","\u001b[K     |████████████████████████████████| 79 kB 7.2 MB/s \n","\u001b[?25hCollecting botocore<1.30.0,>=1.29.27\n","  Downloading botocore-1.29.27-py3-none-any.whl (10.2 MB)\n","\u001b[K     |████████████████████████████████| 10.2 MB 37.6 MB/s \n","\u001b[?25hCollecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n","  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n","\u001b[K     |████████████████████████████████| 127 kB 66.9 MB/s \n","\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.8/dist-packages (from botocore<1.30.0,>=1.29.27->boto3->neuralcoref==4.0) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.27->boto3->neuralcoref==4.0) (1.15.0)\n","Installing collected packages: urllib3, jmespath, botocore, s3transfer, boto3, neuralcoref\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","  Running setup.py develop for neuralcoref\n","Successfully installed boto3-1.26.27 botocore-1.29.27 jmespath-1.0.1 neuralcoref-4.0 s3transfer-0.6.0 urllib3-1.25.11\n"]}]},{"cell_type":"code","source":["import pathlib\n","import random\n","import string\n","import re\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow.keras.layers import TextVectorization"],"metadata":{"id":"9ci9GuKxDGyO","executionInfo":{"status":"ok","timestamp":1670697467704,"user_tz":300,"elapsed":4250,"user":{"displayName":"Linette Kunin","userId":"07994646427956674729"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["## Downloading the data\n","\n","We'll be working with an English-to-Spanish translation dataset\n","provided by [Anki](https://www.manythings.org/anki/). Let's download it:"],"metadata":{"id":"8vdoYt9gSsnH"}},{"cell_type":"code","source":["text_file = keras.utils.get_file(\n","    fname=\"spa-eng.zip\",\n","    origin=\"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\",\n","    extract=True,\n",")\n","text_file = pathlib.Path(text_file).parent / \"spa-eng\" / \"spa.txt\""],"metadata":{"id":"JaDJglFSStZL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670697475625,"user_tz":300,"elapsed":518,"user":{"displayName":"Linette Kunin","userId":"07994646427956674729"}},"outputId":"e48be2f7-c8fd-4465-c72e-9414cf594521"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n","2638744/2638744 [==============================] - 0s 0us/step\n"]}]},{"cell_type":"markdown","source":["## Parsing the data\n","\n","Each line contains an English sentence and its corresponding Spanish sentence.\n","The English sentence is the *source sequence* and Spanish one is the *target sequence*.\n","We prepend the token `\"[start]\"` and we append the token `\"[end]\"` to the Spanish sentence."],"metadata":{"id":"1wwH_2yKSxHV"}},{"cell_type":"code","source":["feminine_markers = {'Ella', 'ella', 'Ellas', 'ellas', 'Mary', 'mary'}\n","masculine_markers = {'Él', 'él', 'Ellos', 'ellos', 'Tom', 'tom', 'Leo', 'leo'}"],"metadata":{"id":"Ela6kMH8s49U","executionInfo":{"status":"ok","timestamp":1670697506916,"user_tz":300,"elapsed":132,"user":{"displayName":"Linette Kunin","userId":"07994646427956674729"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["with open(text_file) as f:\n","    lines = f.read().split(\"\\n\")[:-1]\n","text_pairs = []\n","masculine_counter, feminine_counter = 0, 0\n","for line in lines:\n","    eng, spa = line.split(\"\\t\")\n","    spa_set = set(spa.split())\n","    if feminine_markers.intersection(spa_set): feminine_counter += 1\n","    if masculine_markers.intersection(spa_set): masculine_counter += 1\n","\n","    spa = \"[start] \" + spa + \" [end]\"\n","    text_pairs.append((eng, spa))"],"metadata":{"id":"3MjylxTjSzQ-","executionInfo":{"status":"ok","timestamp":1670697509031,"user_tz":300,"elapsed":497,"user":{"displayName":"Linette Kunin","userId":"07994646427956674729"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["print(f\"Feminine ratio = {feminine_counter/(feminine_counter + masculine_counter)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7FwuEBJivZAI","executionInfo":{"status":"ok","timestamp":1670697511231,"user_tz":300,"elapsed":210,"user":{"displayName":"Linette Kunin","userId":"07994646427956674729"}},"outputId":"ea0d579a-eaf3-46a2-d2e8-066df9de5d3a"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Feminine ratio = 0.21570420695061407\n"]}]},{"cell_type":"markdown","source":["Here's what our sentence pairs look like:"],"metadata":{"id":"Wb-z9nhlS_cc"}},{"cell_type":"code","source":["for _ in range(5):\n","    print(random.choice(text_pairs))"],"metadata":{"id":"n2LuQ-moTBxf","executionInfo":{"status":"ok","timestamp":1670697513891,"user_tz":300,"elapsed":114,"user":{"displayName":"Linette Kunin","userId":"07994646427956674729"}},"outputId":"c2243aab-018d-47a0-9b57-4df6f9129268","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["(\"I'm proud of my father being a good cook.\", '[start] Estoy orgulloso de que mi padre sea un buen cocinero. [end]')\n","('He is well spoken of by everybody.', '[start] Él tiene buena fama con todos. [end]')\n","(\"I don't understand at all.\", '[start] No entiendo para nada. [end]')\n","('I must hurry to class.', '[start] Me tengo que apurar para ir a clases. [end]')\n","('Please put your cigarette out.', '[start] Por favor, apague su cigarrillo. [end]')\n"]}]},{"cell_type":"markdown","source":["Now, let's split the sentence pairs into a training set, a validation set,\n","and a test set."],"metadata":{"id":"xqaH7BVDTDI1"}},{"cell_type":"code","source":["random.shuffle(text_pairs)\n","num_val_samples = int(0.15 * len(text_pairs))\n","num_train_samples = len(text_pairs) - 2 * num_val_samples\n","train_pairs = text_pairs[:num_train_samples]\n","val_pairs = text_pairs[num_train_samples : num_train_samples + num_val_samples]\n","test_pairs = text_pairs[num_train_samples + num_val_samples :]\n","\n","print(f\"{len(text_pairs)} total pairs\")\n","print(f\"{len(train_pairs)} training pairs\")\n","print(f\"{len(val_pairs)} validation pairs\")\n","print(f\"{len(test_pairs)} test pairs\")"],"metadata":{"id":"jDJcO4gqTG-O","executionInfo":{"status":"ok","timestamp":1670697516025,"user_tz":300,"elapsed":167,"user":{"displayName":"Linette Kunin","userId":"07994646427956674729"}},"outputId":"65b165cf-dbc1-42a2-d72f-cc7563c0fad5","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["118964 total pairs\n","83276 training pairs\n","17844 validation pairs\n","17844 test pairs\n"]}]},{"cell_type":"markdown","source":["## Vectorizing the text data\n","\n","We'll use two instances of the `TextVectorization` layer to vectorize the text\n","data (one for English and one for Spanish),\n","that is to say, to turn the original strings into integer sequences\n","where each integer represents the index of a word in a vocabulary.\n","\n","The English layer will use the default string standardization (strip punctuation characters)\n","and splitting scheme (split on whitespace), while\n","the Spanish layer will use a custom standardization, where we add the character\n","`\"¿\"` to the set of punctuation characters to be stripped.\n","\n","Note: in a production-grade machine translation model, I would not recommend\n","stripping the punctuation characters in either language. Instead, I would recommend turning\n","each punctuation character into its own token,\n","which you could achieve by providing a custom `split` function to the `TextVectorization` layer."],"metadata":{"id":"fn7Ju9OOTLmp"}},{"cell_type":"code","source":["strip_chars = string.punctuation + \"¿\"\n","strip_chars = strip_chars.replace(\"[\", \"\")\n","strip_chars = strip_chars.replace(\"]\", \"\")\n","\n","vocab_size = 15000\n","sequence_length = 20\n","batch_size = 64\n","\n","\n","def custom_standardization(input_string):\n","    lowercase = tf.strings.lower(input_string)\n","    return tf.strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")\n","\n","\n","eng_vectorization = TextVectorization(\n","    max_tokens=vocab_size, output_mode=\"int\", output_sequence_length=sequence_length,\n",")\n","spa_vectorization = TextVectorization(\n","    max_tokens=vocab_size,\n","    output_mode=\"int\",\n","    output_sequence_length=sequence_length + 1,\n","    standardize=custom_standardization,\n",")\n","train_eng_texts = [pair[0] for pair in train_pairs]\n","train_spa_texts = [pair[1] for pair in train_pairs]\n","eng_vectorization.adapt(train_eng_texts)\n","spa_vectorization.adapt(train_spa_texts)"],"metadata":{"id":"op5NTCHXTNKQ","executionInfo":{"status":"ok","timestamp":1670697531162,"user_tz":300,"elapsed":11510,"user":{"displayName":"Linette Kunin","userId":"07994646427956674729"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["Next, we'll format our datasets.\n","\n","At each training step, the model will seek to predict target words N+1 (and beyond)\n","using the source sentence and the target words 0 to N.\n","\n","As such, the training dataset will yield a tuple `(inputs, targets)`, where:\n","\n","- `inputs` is a dictionary with the keys `encoder_inputs` and `decoder_inputs`.\n","`encoder_inputs` is the vectorized source sentence and `encoder_inputs` is the target sentence \"so far\",\n","that is to say, the words 0 to N used to predict word N+1 (and beyond) in the target sentence.\n","- `target` is the target sentence offset by one step:\n","it provides the next words in the target sentence -- what the model will try to predict."],"metadata":{"id":"2VYzYa3DTZKw"}},{"cell_type":"code","source":["def format_dataset(eng, spa):\n","    eng = eng_vectorization(eng)\n","    spa = spa_vectorization(spa)\n","    return ({\"encoder_inputs\": eng, \"decoder_inputs\": spa[:, :-1],}, spa[:, 1:])\n","\n","\n","def make_dataset(pairs):\n","    eng_texts, spa_texts = zip(*pairs)\n","    eng_texts = list(eng_texts)\n","    spa_texts = list(spa_texts)\n","    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n","    dataset = dataset.batch(batch_size)\n","    dataset = dataset.map(format_dataset)\n","    return dataset.shuffle(2048).prefetch(16).cache()\n","\n","\n","train_ds = make_dataset(train_pairs)\n","val_ds = make_dataset(val_pairs)"],"metadata":{"id":"n_8ffCvTThbE","executionInfo":{"status":"ok","timestamp":1670697594941,"user_tz":300,"elapsed":1497,"user":{"displayName":"Linette Kunin","userId":"07994646427956674729"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["Let's take a quick look at the sequence shapes\n","(we have batches of 64 pairs, and all sequences are 20 steps long):"],"metadata":{"id":"3loHRGzyTtXT"}},{"cell_type":"markdown","source":[],"metadata":{"id":"DU0J8O6AT6jS"}},{"cell_type":"code","source":["for inputs, targets in train_ds.take(1):\n","    print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')\n","    print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')\n","    print(f\"targets.shape: {targets.shape}\")"],"metadata":{"id":"csEcLEYBT7M7","executionInfo":{"status":"ok","timestamp":1670697599517,"user_tz":300,"elapsed":1403,"user":{"displayName":"Linette Kunin","userId":"07994646427956674729"}},"outputId":"bd2a92a2-f3f4-4c69-d349-7b084c67f4e0","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["inputs[\"encoder_inputs\"].shape: (64, 20)\n","inputs[\"decoder_inputs\"].shape: (64, 20)\n","targets.shape: (64, 20)\n"]}]},{"cell_type":"markdown","source":["## Building the model\n","\n","Our sequence-to-sequence Transformer consists of a `TransformerEncoder`\n","and a `TransformerDecoder` chained together. To make the model aware of word order,\n","we also use a `PositionalEmbedding` layer.\n","\n","The source sequence will be pass to the `TransformerEncoder`,\n","which will produce a new representation of it.\n","This new representation will then be passed\n","to the `TransformerDecoder`, together with the target sequence so far (target words 0 to N).\n","The `TransformerDecoder` will then seek to predict the next words in the target sequence (N+1 and beyond).\n","\n","A key detail that makes this possible is causal masking\n","(see method `get_causal_attention_mask()` on the `TransformerDecoder`).\n","The `TransformerDecoder` sees the entire sequences at once, and thus we must make\n","sure that it only uses information from target tokens 0 to N when predicting token N+1\n","(otherwise, it could use information from the future, which would\n","result in a model that cannot be used at inference time).\n"],"metadata":{"id":"olPbNVLEUBRw"}},{"cell_type":"code","source":["class TransformerEncoder(layers.Layer):\n","    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n","        super(TransformerEncoder, self).__init__(**kwargs)\n","        self.embed_dim = embed_dim\n","        self.dense_dim = dense_dim\n","        self.num_heads = num_heads\n","        self.attention = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=embed_dim\n","        )\n","        self.dense_proj = keras.Sequential(\n","            [layers.Dense(dense_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n","        )\n","        self.layernorm_1 = layers.LayerNormalization()\n","        self.layernorm_2 = layers.LayerNormalization()\n","        self.supports_masking = True\n","\n","    def call(self, inputs, mask=None):\n","        if mask is not None:\n","            padding_mask = tf.cast(mask[:, tf.newaxis, tf.newaxis, :], dtype=\"int32\")\n","        attention_output = self.attention(\n","            query=inputs, value=inputs, key=inputs, attention_mask=padding_mask\n","        )\n","        proj_input = self.layernorm_1(inputs + attention_output)\n","        proj_output = self.dense_proj(proj_input)\n","        return self.layernorm_2(proj_input + proj_output)\n","    def get_config(self):\n","        config = super().get_config()\n","        config.update({\n","            \"embed_dim\": self.embed_dim,\n","            \"dense_dim\": self.dense_dim,\n","            \"num_heads\": self.num_heads,\n","        })\n","        return config\n","\n","\n","class PositionalEmbedding(layers.Layer):\n","    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n","        super(PositionalEmbedding, self).__init__(**kwargs)\n","        self.token_embeddings = layers.Embedding(\n","            input_dim=vocab_size, output_dim=embed_dim\n","        )\n","        self.position_embeddings = layers.Embedding(\n","            input_dim=sequence_length, output_dim=embed_dim\n","        )\n","        self.sequence_length = sequence_length\n","        self.vocab_size = vocab_size\n","        self.embed_dim = embed_dim\n","\n","    def call(self, inputs):\n","        length = tf.shape(inputs)[-1]\n","        positions = tf.range(start=0, limit=length, delta=1)\n","        embedded_tokens = self.token_embeddings(inputs)\n","        embedded_positions = self.position_embeddings(positions)\n","        return embedded_tokens + embedded_positions\n","\n","    def compute_mask(self, inputs, mask=None):\n","        return tf.math.not_equal(inputs, 0)\n","    def get_config(self):\n","        config = super().get_config()\n","        config.update({\n","            \"sequence_length\": self.sequence_length,\n","            \"vocab_size\": self.vocab_size,\n","            \"embed_dim\": self.embed_dim,\n","        })\n","        return config\n","\n","\n","class TransformerDecoder(layers.Layer):\n","    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n","        super(TransformerDecoder, self).__init__(**kwargs)\n","        self.embed_dim = embed_dim\n","        self.latent_dim = latent_dim\n","        self.num_heads = num_heads\n","        self.attention_1 = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=embed_dim\n","        )\n","        self.attention_2 = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=embed_dim\n","        )\n","        self.dense_proj = keras.Sequential(\n","            [layers.Dense(latent_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n","        )\n","        self.layernorm_1 = layers.LayerNormalization()\n","        self.layernorm_2 = layers.LayerNormalization()\n","        self.layernorm_3 = layers.LayerNormalization()\n","        self.supports_masking = True\n","\n","    def call(self, inputs, encoder_outputs, mask=None):\n","        causal_mask = self.get_causal_attention_mask(inputs)\n","        if mask is not None:\n","            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n","            padding_mask = tf.minimum(padding_mask, causal_mask)\n","\n","        attention_output_1 = self.attention_1(\n","            query=inputs, value=inputs, key=inputs, attention_mask=causal_mask\n","        )\n","        out_1 = self.layernorm_1(inputs + attention_output_1)\n","\n","        attention_output_2 = self.attention_2(\n","            query=out_1,\n","            value=encoder_outputs,\n","            key=encoder_outputs,\n","            attention_mask=padding_mask,\n","        )\n","        out_2 = self.layernorm_2(out_1 + attention_output_2)\n","\n","        proj_output = self.dense_proj(out_2)\n","        return self.layernorm_3(out_2 + proj_output)\n","\n","    def get_causal_attention_mask(self, inputs):\n","        input_shape = tf.shape(inputs)\n","        batch_size, sequence_length = input_shape[0], input_shape[1]\n","        i = tf.range(sequence_length)[:, tf.newaxis]\n","        j = tf.range(sequence_length)\n","        mask = tf.cast(i >= j, dtype=\"int32\")\n","        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n","        mult = tf.concat(\n","            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n","            axis=0,\n","        )\n","        return tf.tile(mask, mult)\n","    def get_config(self):\n","        config = super().get_config()\n","        config.update({\n","            \"embed_dim\": self.embed_dim,\n","            \"latent_dim\": self.latent_dim,\n","            \"num_heads\": self.num_heads,\n","        })\n","        return config"],"metadata":{"id":"ZXa-hgFpULRp","executionInfo":{"status":"ok","timestamp":1670697609063,"user_tz":300,"elapsed":189,"user":{"displayName":"Linette Kunin","userId":"07994646427956674729"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["embed_dim = 256\n","latent_dim = 2048\n","num_heads = 8\n","\n","encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n","x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n","encoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(x)\n","encoder = keras.Model(encoder_inputs, encoder_outputs)\n","\n","decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n","encoded_seq_inputs = keras.Input(shape=(None, embed_dim), name=\"decoder_state_inputs\")\n","x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n","x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs)\n","x = layers.Dropout(0.5)(x)\n","decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n","decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n","\n","decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n","transformer = keras.Model(\n","    [encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\"\n",")"],"metadata":{"id":"iB1VrLfjUpnF","executionInfo":{"status":"ok","timestamp":1670697616839,"user_tz":300,"elapsed":2673,"user":{"displayName":"Linette Kunin","userId":"07994646427956674729"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["epochs = 5  # This should be at least 30 for convergence\n","\n","transformer.summary()\n","transformer.compile(\n","    \"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",")\n","transformer.fit(train_ds, epochs=epochs, validation_data=val_ds)"],"metadata":{"id":"KYCElnuUUy0K","outputId":"0a3f026e-8094-4594-b0ec-bcc3ccd861c9","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"transformer\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," encoder_inputs (InputLayer)    [(None, None)]       0           []                               \n","                                                                                                  \n"," positional_embedding (Position  (None, None, 256)   3845120     ['encoder_inputs[0][0]']         \n"," alEmbedding)                                                                                     \n","                                                                                                  \n"," decoder_inputs (InputLayer)    [(None, None)]       0           []                               \n","                                                                                                  \n"," transformer_encoder (Transform  (None, None, 256)   3155456     ['positional_embedding[0][0]']   \n"," erEncoder)                                                                                       \n","                                                                                                  \n"," model_1 (Functional)           (None, None, 15000)  12959640    ['decoder_inputs[0][0]',         \n","                                                                  'transformer_encoder[0][0]']    \n","                                                                                                  \n","==================================================================================================\n","Total params: 19,960,216\n","Trainable params: 19,960,216\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","Epoch 1/5\n","1302/1302 [==============================] - 3731s 3s/step - loss: 1.6476 - accuracy: 0.4287 - val_loss: 1.3235 - val_accuracy: 0.5205\n","Epoch 2/5\n","1302/1302 [==============================] - 3650s 3s/step - loss: 1.3114 - accuracy: 0.5436 - val_loss: 1.1514 - val_accuracy: 0.5753\n","Epoch 3/5\n","1302/1302 [==============================] - 3598s 3s/step - loss: 1.1612 - accuracy: 0.5908 - val_loss: 1.0746 - val_accuracy: 0.6065\n","Epoch 4/5\n","1302/1302 [==============================] - 3590s 3s/step - loss: 1.0765 - accuracy: 0.6217 - val_loss: 1.0323 - val_accuracy: 0.6278\n","Epoch 5/5\n","1302/1302 [==============================] - ETA: 0s - loss: 1.0311 - accuracy: 0.6423"]}]},{"cell_type":"code","source":["spa_vocab = spa_vectorization.get_vocabulary()\n","spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n","max_decoded_sentence_length = 20\n","\n","\n","def decode_sequence(input_sentence):\n","    tokenized_input_sentence = eng_vectorization([input_sentence])\n","    decoded_sentence = \"[start]\"\n","    for i in range(max_decoded_sentence_length):\n","        tokenized_target_sentence = spa_vectorization([decoded_sentence])[:, :-1]\n","        predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n","\n","        sampled_token_index = np.argmax(predictions[0, i, :])\n","        sampled_token = spa_index_lookup[sampled_token_index]\n","        decoded_sentence += \" \" + sampled_token\n","\n","        if sampled_token == \"[end]\":\n","            break\n","    return decoded_sentence\n","\n","\n","test_eng_texts = [pair[0] for pair in test_pairs]\n","for _ in range(30):\n","    input_sentence = random.choice(test_eng_texts)\n","    translated = decode_sequence(input_sentence)\n","    print(input_sentence, translated)"],"metadata":{"id":"I2C4CPliU4zM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670715902373,"user_tz":300,"elapsed":175,"user":{"displayName":"Linette Kunin","userId":"07994646427956674729"}},"outputId":"d30490ae-5ed8-44a3-e099-8c3da4c9f2f3"},"execution_count":17,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["I didn't kiss Tom. [start] no [UNK] a tom [end]\n","The almond trees are in bloom. [start] los [UNK] están en [UNK] [end]\n","Tom outlived Mary. [start] tom [UNK] a mary [end]\n","I wish you didn't do that. [start] ojalá no lo hizo [end]\n","I want a small bunch of grapes. [start] quiero un poco de [UNK] [end]\n","What're you going to do during your summer vacation? [start] qué vas a hacer por verano [end]\n","I want everyone in my office in 20 minutes. [start] quiero en mi oficina en el año estado minutos [end]\n","My English teacher advised me to read these books. [start] mi profesor de profesor me aconsejó que se me aconsejó leer libros [end]\n","The well being of the nation is the government's responsibility. [start] el bien que está bien la [UNK] es el [UNK] de la [UNK] [end]\n","Are you going to kiss me or not? [start] vas a [UNK] o no [end]\n","I wanted to be a journalist. [start] quería ser un clase [end]\n","Tom saw someone standing outside his window. [start] tom vio alguien cerca de su ventana [end]\n","Are you feeling OK? [start] te estás bien [end]\n","What are some foods you shouldn't eat if you are pregnant? [start] qué hay algo de dinero que no deberías comer [end]\n","Tom seems dangerous. [start] tom parece peligroso [end]\n","I want to sing the song. [start] quiero cantar esa canción [end]\n","They're not mine. [start] no son la mía [end]\n","Aren't they Americans? [start] no son los animales [end]\n","I thought it was a lot of fun. [start] pensé que era mucho de [UNK] [end]\n","Tom usually gets home at about six o'clock. [start] tom se [UNK] en casa a las seis [end]\n","The death penalty should be abolished. [start] la muerte debería ser [UNK] [end]\n","I thought it was unnecessary for us to do anything about that today. [start] pensé que fue [UNK] para que [UNK] algo de hoy [end]\n","What's on TV? [start] qué hay en la televisión [end]\n","Who is on the train? [start] quién está en el tren [end]\n","I have recently given up smoking. [start] he pasado de [UNK] [end]\n","Tom seems friendlier than before. [start] tom parece que antes [end]\n","Miniskirts have gone out of fashion. [start] las [UNK] se ha ido de [UNK] [end]\n","I have been reading this book. [start] he estado ocupada siempre [end]\n","When did your father come home? [start] cuándo se llama tu padre [end]\n","Tom was a little upset. [start] tom estaba un poco poco [end]\n"]}]},{"cell_type":"code","source":["import numpy as np\n","from nltk.translate.bleu_score import sentence_bleu\n","translated_sentences = []\n","for i in range(30):\n","  input_sentence = test_eng_texts[i]\n","  translated = decode_sequence(input_sentence)\n","  translated_sentences.append(translated)\n","\n","test_sp_texts = [pair[1] for pair in test_pairs]\n","\n","bleu_scores = []\n","for j in range(len(translated_sentences)):\n","  bleu_scores.append(sentence_bleu(translated_sentences[j], test_sp_texts[j], weights = (1, 0, 0, 0)))\n","  print(translated_sentences[j], test_sp_texts[j] )\n","print(bleu_scores) \n","print(np.sum(np.array(bleu_scores))/len(bleu_scores))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ca-iNAAtdNSa","executionInfo":{"status":"ok","timestamp":1670715913992,"user_tz":300,"elapsed":11619,"user":{"displayName":"Linette Kunin","userId":"07994646427956674729"}},"outputId":"ac5a9ea6-c208-4c2a-bdf7-98f3380570bf"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["[start] parece que el policía en este policía es un poco de [UNK] el agua de su [UNK] [end] [start] Parece ser que el policía de esta serie de televisión es un policía corrupto que abusa de su autoridad. [end]\n","[start] me había un accidente el año pasado [end] [start] Tuve un infarto el año pasado. [end]\n","[start] tom no cree que sabe perder [end] [start] Tom no cree que pueda ganar. [end]\n","[start] tom me pidió que [UNK] la música [end] [start] Tom nos pidió bajar la música. [end]\n","[start] ella se [UNK] una enfermedad de una enfermedad [end] [start] Ella sufre de una enfermedad respiratoria. [end]\n","[start] has visto a tu madre [end] [start] ¿Le has mentido alguna vez a tu madre? [end]\n","[start] por favor [UNK] el teléfono a su lengua [end] [start] Recoja su boleto en el mostrador, por favor. [end]\n","[start] ellos no tienen nada que comer [end] [start] Ellos no tienen nada para comer. [end]\n","[start] ni siquiera estamos a quién va a estar allí [end] [start] Ni siquiera sabemos quién estará allí. [end]\n","[start] sabía que estaban vivo [end] [start] Sabía que estabas vivo. [end]\n","[start] por favor [UNK] a los otros [end] [start] Traiga a los otros, por favor. [end]\n","[start] eso es que te estoy seguro de que no ir solo [end] [start] Por eso te estoy diciendo que no vayas solo. [end]\n","[start] no te [UNK] la televisión [end] [start] No dejes la televisión encendida. [end]\n","[start] muchos cosas fueron [UNK] a japón [end] [start] Muchas cosas llegaron a Japón desde China. [end]\n","[start] [UNK] [UNK] este [UNK] por mes pasado [end] [start] La producción de acero crecerá un 2% con respecto al mes anterior. [end]\n","[start] por qué comprar flores es lo que solo dijo que dijiste es muy capaz de que no puedo ver nada [start] ¿Por qué compré flores? ¿Es eso lo que tú dijiste? Está verdaderamente ruidoso aquí, así que no te oí muy bien. [end]\n","[start] cuál es la oportunidad de un [UNK] y un abogado [end] [start] ¿Cuál es la diferencia entre una estrella y un planeta? [end]\n","[start] empezó a llover [end] [start] Empezó a llover fuerte. [end]\n","[start] los [UNK] de los [UNK] [end] [start] Los monos se suben a los árboles. [end]\n","[start] espero que te [UNK] aquí [end] [start] Espero que estén cómodos aquí. [end]\n","[start] me había algo más [end] [start] Tenía otra cosa en la cabeza. [end]\n","[start] el hombre mayor vino con un poco [end] [start] El anciano caminaba con un bastón. [end]\n","[start] Él estaba [UNK] en este [UNK] [end] [start] Él fue sepultado en este cementerio. [end]\n","[start] este problema es demasiado difícil para mí para mí [end] [start] Este problema es demasiado difícil para que yo lo resuelva. [end]\n","[start] estaba tarde [end] [start] Llegué tarde. [end]\n","[start] ayer se comió el tren [end] [start] Ayer él perdió el tren a Sapporo. [end]\n","[start] Él tiene mucho dinero más [end] [start] Él tiene mucho más dinero que yo. [end]\n","[start] me [UNK] a la fiesta [end] [start] Ellos me invitaron a la fiesta. [end]\n","[start] no [UNK] inglés por favor [end] [start] Por favor no uses inglés. [end]\n","[start] cuando estás a las diez [end] [start] Cuando estés enfadado, cuenta hasta diez. [end]\n","[0.15384615384615385, 0.36363636363636365, 0.38095238095238093, 0.4090909090909091, 0.25, 0.30769230769230765, 0.27586206896551724, 0.3260869565217391, 0.3461538461538461, 0.4594594594594595, 0.3409090909090909, 0.27586206896551724, 0.34042553191489366, 0.2857142857142857, 0.1625, 0.184, 0.2463768115942029, 0.4594594594594595, 0.2553191489361702, 0.3409090909090909, 0.32558139534883723, 0.3541666666666667, 0.24, 0.2602739726027397, 0.37037037037037035, 0.31914893617021284, 0.40425531914893614, 0.31111111111111106, 0.4358974358974359, 0.2727272727272727]\n","0.3152596138254991\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n","The hypothesis contains 0 counts of 2-gram overlaps.\n","Therefore the BLEU score evaluates to 0, independently of\n","how many N-gram overlaps of lower order it contains.\n","Consider using lower n-gram order or use SmoothingFunction()\n","  warnings.warn(_msg)\n","/usr/local/lib/python3.8/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n","The hypothesis contains 0 counts of 3-gram overlaps.\n","Therefore the BLEU score evaluates to 0, independently of\n","how many N-gram overlaps of lower order it contains.\n","Consider using lower n-gram order or use SmoothingFunction()\n","  warnings.warn(_msg)\n","/usr/local/lib/python3.8/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n","The hypothesis contains 0 counts of 4-gram overlaps.\n","Therefore the BLEU score evaluates to 0, independently of\n","how many N-gram overlaps of lower order it contains.\n","Consider using lower n-gram order or use SmoothingFunction()\n","  warnings.warn(_msg)\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"VG5SETKSSj8N"},"execution_count":null,"outputs":[]}]}