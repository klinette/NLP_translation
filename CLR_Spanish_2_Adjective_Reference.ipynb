{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xKax1961_RY6","outputId":"6d55906d-e7e9-4eaa-cfb0-148be0c0342d","executionInfo":{"status":"ok","timestamp":1669074513198,"user_tz":300,"elapsed":5416,"user":{"displayName":"Catherine Mei","userId":"11274645310279339284"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["Cloning into 'neuralcoref'...\n","Checking out files:  99% (150/151)   \rChecking out files: 100% (151/151)   \rChecking out files: 100% (151/151), done.\n"]}],"source":["%%bash\n","!(stat -t /usr/local/lib/*/dist-packages/google/colab > /dev/null 2>&1) && exit \n","git clone https://github.com/huggingface/neuralcoref.git"]},{"cell_type":"code","source":["# !git clone https://github.com/huggingface/neuralcoref.git\n","!pip install https://huggingface.co/spacy/en_core_web_sm/resolve/main/en_core_web_sm-any-py3-none-any.whl"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8l2LVMF3R1YM","executionInfo":{"status":"ok","timestamp":1669074525272,"user_tz":300,"elapsed":8893,"user":{"displayName":"Catherine Mei","userId":"11274645310279339284"}},"outputId":"1628ddde-1e2c-47a7-e2ab-51347a0348e7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting en-core-web-sm==any\n","  Downloading https://huggingface.co/spacy/en_core_web_sm/resolve/main/en_core_web_sm-any-py3-none-any.whl (12.8 MB)\n","\u001b[K     |████████████████████████████████| 12.8 MB 1.7 MB/s \n","\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==any) (3.4.3)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==any) (2.23.0)\n","Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==any) (0.10.1)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==any) (3.3.0)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==any) (3.0.10)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==any) (1.0.3)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==any) (3.0.8)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==any) (2.11.3)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==any) (8.1.5)\n","Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==any) (0.7.0)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==any) (2.4.5)\n","Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==any) (4.1.1)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==any) (21.3)\n","Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==any) (0.8.1)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==any) (2.0.8)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==any) (2.0.7)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==any) (4.64.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==any) (57.4.0)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==any) (1.10.2)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==any) (1.21.6)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==any) (1.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.5.0,>=3.4.0->en-core-web-sm==any) (3.10.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==any) (3.0.9)\n","Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==any) (5.2.1)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==any) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==any) (2022.9.24)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==any) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==any) (1.24.3)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==any) (0.7.9)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==any) (0.0.3)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==any) (7.1.2)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==any) (2.0.1)\n","Installing collected packages: en-core-web-sm\n","  Attempting uninstall: en-core-web-sm\n","    Found existing installation: en-core-web-sm 3.4.1\n","    Uninstalling en-core-web-sm-3.4.1:\n","      Successfully uninstalled en-core-web-sm-3.4.1\n","Successfully installed en-core-web-sm-3.4.0\n"]}]},{"cell_type":"code","source":["!pip install -U spacy"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZSoQ8h6iQfGQ","executionInfo":{"status":"ok","timestamp":1669074531539,"user_tz":300,"elapsed":6275,"user":{"displayName":"Catherine Mei","userId":"11274645310279339284"}},"outputId":"ee4ff313-0170-4670-d75a-c881d2b636cb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (3.4.3)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.3.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.3)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.10)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.8)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.8)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (21.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n","Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.10.1)\n","Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.1.1)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.21.6)\n","Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.7.0)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.10.2)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.9)\n","Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.1)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.7)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (8.1.5)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.64.1)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.4.5)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy) (3.10.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (3.0.9)\n","Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy) (5.2.1)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.9.24)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.9)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.3)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.8.0,>=0.3.0->spacy) (7.1.2)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (2.0.1)\n"]}]},{"cell_type":"code","source":["%cd neuralcoref\n","\n","!pip install -r requirements.txt\n","!pip install -e ."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VJzyUVKQQh2a","executionInfo":{"status":"ok","timestamp":1669074589113,"user_tz":300,"elapsed":57599,"user":{"displayName":"Catherine Mei","userId":"11274645310279339284"}},"outputId":"7017d406-28b6-4ed5-fa86-fe41b9d8e9d3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/neuralcoref\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting spacy<3.0.0,>=2.1.0\n","  Downloading spacy-2.3.8-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n","\u001b[K     |████████████████████████████████| 4.8 MB 5.3 MB/s \n","\u001b[?25hRequirement already satisfied: cython>=0.25 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (0.29.32)\n","Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (3.6.4)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (1.0.9)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (3.0.8)\n","Collecting srsly<1.1.0,>=1.0.2\n","  Downloading srsly-1.0.6-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (208 kB)\n","\u001b[K     |████████████████████████████████| 208 kB 45.6 MB/s \n","\u001b[?25hRequirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (0.10.1)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (1.21.6)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (4.64.1)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (2.23.0)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (2.0.7)\n","Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (0.7.9)\n","Collecting thinc<7.5.0,>=7.4.1\n","  Downloading thinc-7.4.6-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n","\u001b[K     |████████████████████████████████| 1.0 MB 49.5 MB/s \n","\u001b[?25hCollecting catalogue<1.1.0,>=0.0.7\n","  Downloading catalogue-1.0.2-py2.py3-none-any.whl (16 kB)\n","Collecting plac<1.2.0,>=0.9.6\n","  Downloading plac-1.1.3-py2.py3-none-any.whl (20 kB)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (57.4.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (3.10.0)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (4.1.1)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (2022.9.24)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (3.0.4)\n","Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->-r requirements.txt (line 3)) (1.11.0)\n","Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->-r requirements.txt (line 3)) (0.7.1)\n","Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest->-r requirements.txt (line 3)) (9.0.0)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from pytest->-r requirements.txt (line 3)) (1.15.0)\n","Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest->-r requirements.txt (line 3)) (22.1.0)\n","Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->-r requirements.txt (line 3)) (1.4.1)\n","Installing collected packages: srsly, plac, catalogue, thinc, spacy\n","  Attempting uninstall: srsly\n","    Found existing installation: srsly 2.4.5\n","    Uninstalling srsly-2.4.5:\n","      Successfully uninstalled srsly-2.4.5\n","  Attempting uninstall: catalogue\n","    Found existing installation: catalogue 2.0.8\n","    Uninstalling catalogue-2.0.8:\n","      Successfully uninstalled catalogue-2.0.8\n","  Attempting uninstall: thinc\n","    Found existing installation: thinc 8.1.5\n","    Uninstalling thinc-8.1.5:\n","      Successfully uninstalled thinc-8.1.5\n","  Attempting uninstall: spacy\n","    Found existing installation: spacy 3.4.3\n","    Uninstalling spacy-3.4.3:\n","      Successfully uninstalled spacy-3.4.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","en-core-web-sm 3.4.0 requires spacy<3.5.0,>=3.4.0, but you have spacy 2.3.8 which is incompatible.\n","confection 0.0.3 requires srsly<3.0.0,>=2.4.0, but you have srsly 1.0.6 which is incompatible.\u001b[0m\n","Successfully installed catalogue-1.0.2 plac-1.1.3 spacy-2.3.8 srsly-1.0.6 thinc-7.4.6\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Obtaining file:///content/neuralcoref\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from neuralcoref==4.0) (1.21.6)\n","Collecting boto3\n","  Downloading boto3-1.26.14-py3-none-any.whl (132 kB)\n","\u001b[K     |████████████████████████████████| 132 kB 6.0 MB/s \n","\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from neuralcoref==4.0) (2.23.0)\n","Requirement already satisfied: spacy<3.0.0,>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from neuralcoref==4.0) (2.3.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref==4.0) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref==4.0) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref==4.0) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref==4.0) (2022.9.24)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (1.0.9)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (1.1.3)\n","Requirement already satisfied: thinc<7.5.0,>=7.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (7.4.6)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (1.0.6)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (2.0.7)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (4.64.1)\n","Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (0.7.9)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (57.4.0)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (3.0.8)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (1.0.2)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (0.10.1)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (4.1.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (3.10.0)\n","Collecting botocore<1.30.0,>=1.29.14\n","  Downloading botocore-1.29.14-py3-none-any.whl (9.9 MB)\n","\u001b[K     |████████████████████████████████| 9.9 MB 37.5 MB/s \n","\u001b[?25hCollecting s3transfer<0.7.0,>=0.6.0\n","  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n","\u001b[K     |████████████████████████████████| 79 kB 7.0 MB/s \n","\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n","  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n","Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n","  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n","\u001b[K     |████████████████████████████████| 127 kB 62.8 MB/s \n","\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.30.0,>=1.29.14->boto3->neuralcoref==4.0) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.14->boto3->neuralcoref==4.0) (1.15.0)\n","Installing collected packages: urllib3, jmespath, botocore, s3transfer, boto3, neuralcoref\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","  Running setup.py develop for neuralcoref\n","Successfully installed boto3-1.26.14 botocore-1.29.14 jmespath-1.0.1 neuralcoref-4.0 s3transfer-0.6.0 urllib3-1.25.11\n"]}]},{"cell_type":"code","source":["!python -m spacy download en_core_web_sm"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Hm1iuuCpQgaF","executionInfo":{"status":"ok","timestamp":1669074606273,"user_tz":300,"elapsed":17168,"user":{"displayName":"Catherine Mei","userId":"11274645310279339284"}},"outputId":"d7a98477-67de-4ac5-d20d-fe501d8ae933"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting en_core_web_sm==2.3.1\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz (12.0 MB)\n","\u001b[K     |████████████████████████████████| 12.0 MB 2.8 MB/s \n","\u001b[?25hRequirement already satisfied: spacy<2.4.0,>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.3.1) (2.3.8)\n","Requirement already satisfied: thinc<7.5.0,>=7.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (7.4.6)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.64.1)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.1.3)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.23.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (57.4.0)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.10.1)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.21.6)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.7)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.8)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.2)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.9)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.6)\n","Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.7.9)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.1.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.10.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.25.11)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2022.9.24)\n","Building wheels for collected packages: en-core-web-sm\n","  Building wheel for en-core-web-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for en-core-web-sm: filename=en_core_web_sm-2.3.1-py3-none-any.whl size=12047104 sha256=e9787ced82403c5912ae78f61ad315ed4b9662e7e197334c66941e74654f9635\n","  Stored in directory: /root/.cache/pip/wheels/b7/0d/f0/7ecae8427c515065d75410989e15e5785dd3975fe06e795cd9\n","Successfully built en-core-web-sm\n","Installing collected packages: en-core-web-sm\n","  Attempting uninstall: en-core-web-sm\n","    Found existing installation: en-core-web-sm 3.4.0\n","    Uninstalling en-core-web-sm-3.4.0:\n","      Successfully uninstalled en-core-web-sm-3.4.0\n","Successfully installed en-core-web-sm-2.3.1\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the model via spacy.load('en_core_web_sm')\n"]}]},{"cell_type":"code","source":["import pathlib\n","import random\n","import string\n","import re\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow.keras.layers import TextVectorization"],"metadata":{"id":"9ci9GuKxDGyO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Downloading the data\n","\n","We'll be working with an English-to-Spanish translation dataset\n","provided by [Anki](https://www.manythings.org/anki/). Let's download it:"],"metadata":{"id":"8vdoYt9gSsnH"}},{"cell_type":"code","source":["text_file = keras.utils.get_file(\n","    fname=\"spa-eng.zip\",\n","    origin=\"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\",\n","    extract=True,\n",")\n","text_file = pathlib.Path(text_file).parent / \"spa-eng\" / \"spa.txt\""],"metadata":{"id":"JaDJglFSStZL","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a33c64be-0714-4cfa-fe6b-2bfc2c9effae","executionInfo":{"status":"ok","timestamp":1669074611429,"user_tz":300,"elapsed":159,"user":{"displayName":"Catherine Mei","userId":"11274645310279339284"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n","2638744/2638744 [==============================] - 0s 0us/step\n"]}]},{"cell_type":"code","source":["import spacy\n","from spacy import displacy\n","\n","# https://spacy.io/usage/linguistic-features\n","# https://spacy.io/usage/visualizers\n","\n","nlp = spacy.load('en_core_web_sm')"],"metadata":{"id":"Qa42eXLWPQc8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["doc = nlp(\"A mistake young people often make is to start learning too many languages at the same time, as they underestimate the difficulties and overestimate their own ability to learn them.\")"],"metadata":{"id":"aUCEec7GPyS3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["displacy.render(doc, style=\"dep\", jupyter=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":703},"id":"AmaRuF4sTyE1","executionInfo":{"status":"ok","timestamp":1669076530496,"user_tz":300,"elapsed":175,"user":{"displayName":"Catherine Mei","userId":"11274645310279339284"}},"outputId":"96abe1de-5434-493b-fa01-0b15a9e0fdb5"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"5c7977dc54f14ccf8d672621edb2a146-0\" class=\"displacy\" width=\"5300\" height=\"662.0\" direction=\"ltr\" style=\"max-width: none; height: 662.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">A</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">DET</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">mistake</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">ADJ</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">young</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">ADJ</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">people</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">NOUN</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">often</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">ADV</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">make</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">VERB</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">is</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">AUX</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">to</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">PART</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">start</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">VERB</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1625\">learning</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1625\">VERB</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1800\">too</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1800\">ADV</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1975\">many</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1975\">ADJ</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2150\">languages</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2150\">NOUN</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2325\">at</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2325\">ADP</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2500\">the</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2500\">DET</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2675\">same</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2675\">ADJ</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2850\">time,</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2850\">NOUN</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3025\">as</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3025\">SCONJ</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3200\">they</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3200\">PRON</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3375\">underestimate</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3375\">VERB</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3550\">the</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3550\">DET</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3725\">difficulties</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3725\">NOUN</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3900\">and</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3900\">CCONJ</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"4075\">overestimate</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"4075\">VERB</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"4250\">their</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"4250\">DET</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"4425\">own</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"4425\">ADJ</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"4600\">ability</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"4600\">NOUN</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"4775\">to</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"4775\">PART</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"4950\">learn</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"4950\">VERB</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"5125\">them.</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"5125\">PRON</tspan>\n","</text>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-5c7977dc54f14ccf8d672621edb2a146-0-0\" stroke-width=\"2px\" d=\"M70,527.0 C70,439.5 200.0,439.5 200.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-5c7977dc54f14ccf8d672621edb2a146-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M70,529.0 L62,517.0 78,517.0\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-5c7977dc54f14ccf8d672621edb2a146-0-1\" stroke-width=\"2px\" d=\"M245,527.0 C245,89.5 1095.0,89.5 1095.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-5c7977dc54f14ccf8d672621edb2a146-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M245,529.0 L237,517.0 253,517.0\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-5c7977dc54f14ccf8d672621edb2a146-0-2\" stroke-width=\"2px\" d=\"M420,527.0 C420,439.5 550.0,439.5 550.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-5c7977dc54f14ccf8d672621edb2a146-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M420,529.0 L412,517.0 428,517.0\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-5c7977dc54f14ccf8d672621edb2a146-0-3\" stroke-width=\"2px\" d=\"M595,527.0 C595,352.0 905.0,352.0 905.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-5c7977dc54f14ccf8d672621edb2a146-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M595,529.0 L587,517.0 603,517.0\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-5c7977dc54f14ccf8d672621edb2a146-0-4\" stroke-width=\"2px\" d=\"M770,527.0 C770,439.5 900.0,439.5 900.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-5c7977dc54f14ccf8d672621edb2a146-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M770,529.0 L762,517.0 778,517.0\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-5c7977dc54f14ccf8d672621edb2a146-0-5\" stroke-width=\"2px\" d=\"M245,527.0 C245,177.0 915.0,177.0 915.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-5c7977dc54f14ccf8d672621edb2a146-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">relcl</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M915.0,529.0 L923.0,517.0 907.0,517.0\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-5c7977dc54f14ccf8d672621edb2a146-0-6\" stroke-width=\"2px\" d=\"M1295,527.0 C1295,439.5 1425.0,439.5 1425.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-5c7977dc54f14ccf8d672621edb2a146-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M1295,529.0 L1287,517.0 1303,517.0\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-5c7977dc54f14ccf8d672621edb2a146-0-7\" stroke-width=\"2px\" d=\"M1120,527.0 C1120,352.0 1430.0,352.0 1430.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-5c7977dc54f14ccf8d672621edb2a146-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">xcomp</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M1430.0,529.0 L1438.0,517.0 1422.0,517.0\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-5c7977dc54f14ccf8d672621edb2a146-0-8\" stroke-width=\"2px\" d=\"M1470,527.0 C1470,439.5 1600.0,439.5 1600.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-5c7977dc54f14ccf8d672621edb2a146-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">xcomp</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M1600.0,529.0 L1608.0,517.0 1592.0,517.0\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-5c7977dc54f14ccf8d672621edb2a146-0-9\" stroke-width=\"2px\" d=\"M1820,527.0 C1820,439.5 1950.0,439.5 1950.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-5c7977dc54f14ccf8d672621edb2a146-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M1820,529.0 L1812,517.0 1828,517.0\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-5c7977dc54f14ccf8d672621edb2a146-0-10\" stroke-width=\"2px\" d=\"M1995,527.0 C1995,439.5 2125.0,439.5 2125.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-5c7977dc54f14ccf8d672621edb2a146-0-10\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M1995,529.0 L1987,517.0 2003,517.0\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-5c7977dc54f14ccf8d672621edb2a146-0-11\" stroke-width=\"2px\" d=\"M1645,527.0 C1645,264.5 2135.0,264.5 2135.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-5c7977dc54f14ccf8d672621edb2a146-0-11\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M2135.0,529.0 L2143.0,517.0 2127.0,517.0\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-5c7977dc54f14ccf8d672621edb2a146-0-12\" stroke-width=\"2px\" d=\"M1645,527.0 C1645,177.0 2315.0,177.0 2315.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-5c7977dc54f14ccf8d672621edb2a146-0-12\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M2315.0,529.0 L2323.0,517.0 2307.0,517.0\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-5c7977dc54f14ccf8d672621edb2a146-0-13\" stroke-width=\"2px\" d=\"M2520,527.0 C2520,352.0 2830.0,352.0 2830.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-5c7977dc54f14ccf8d672621edb2a146-0-13\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M2520,529.0 L2512,517.0 2528,517.0\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-5c7977dc54f14ccf8d672621edb2a146-0-14\" stroke-width=\"2px\" d=\"M2695,527.0 C2695,439.5 2825.0,439.5 2825.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-5c7977dc54f14ccf8d672621edb2a146-0-14\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M2695,529.0 L2687,517.0 2703,517.0\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-5c7977dc54f14ccf8d672621edb2a146-0-15\" stroke-width=\"2px\" d=\"M2345,527.0 C2345,264.5 2835.0,264.5 2835.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-5c7977dc54f14ccf8d672621edb2a146-0-15\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M2835.0,529.0 L2843.0,517.0 2827.0,517.0\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-5c7977dc54f14ccf8d672621edb2a146-0-16\" stroke-width=\"2px\" d=\"M3045,527.0 C3045,352.0 3355.0,352.0 3355.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-5c7977dc54f14ccf8d672621edb2a146-0-16\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">mark</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M3045,529.0 L3037,517.0 3053,517.0\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-5c7977dc54f14ccf8d672621edb2a146-0-17\" stroke-width=\"2px\" d=\"M3220,527.0 C3220,439.5 3350.0,439.5 3350.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-5c7977dc54f14ccf8d672621edb2a146-0-17\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M3220,529.0 L3212,517.0 3228,517.0\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-5c7977dc54f14ccf8d672621edb2a146-0-18\" stroke-width=\"2px\" d=\"M1470,527.0 C1470,2.0 3375.0,2.0 3375.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-5c7977dc54f14ccf8d672621edb2a146-0-18\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advcl</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M3375.0,529.0 L3383.0,517.0 3367.0,517.0\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-5c7977dc54f14ccf8d672621edb2a146-0-19\" stroke-width=\"2px\" d=\"M3570,527.0 C3570,439.5 3700.0,439.5 3700.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-5c7977dc54f14ccf8d672621edb2a146-0-19\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M3570,529.0 L3562,517.0 3578,517.0\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-5c7977dc54f14ccf8d672621edb2a146-0-20\" stroke-width=\"2px\" d=\"M3395,527.0 C3395,352.0 3705.0,352.0 3705.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-5c7977dc54f14ccf8d672621edb2a146-0-20\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M3705.0,529.0 L3713.0,517.0 3697.0,517.0\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-5c7977dc54f14ccf8d672621edb2a146-0-21\" stroke-width=\"2px\" d=\"M3395,527.0 C3395,264.5 3885.0,264.5 3885.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-5c7977dc54f14ccf8d672621edb2a146-0-21\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">cc</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M3885.0,529.0 L3893.0,517.0 3877.0,517.0\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-5c7977dc54f14ccf8d672621edb2a146-0-22\" stroke-width=\"2px\" d=\"M3395,527.0 C3395,177.0 4065.0,177.0 4065.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-5c7977dc54f14ccf8d672621edb2a146-0-22\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M4065.0,529.0 L4073.0,517.0 4057.0,517.0\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-5c7977dc54f14ccf8d672621edb2a146-0-23\" stroke-width=\"2px\" d=\"M4270,527.0 C4270,352.0 4580.0,352.0 4580.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-5c7977dc54f14ccf8d672621edb2a146-0-23\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">poss</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M4270,529.0 L4262,517.0 4278,517.0\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-5c7977dc54f14ccf8d672621edb2a146-0-24\" stroke-width=\"2px\" d=\"M4445,527.0 C4445,439.5 4575.0,439.5 4575.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-5c7977dc54f14ccf8d672621edb2a146-0-24\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M4445,529.0 L4437,517.0 4453,517.0\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-5c7977dc54f14ccf8d672621edb2a146-0-25\" stroke-width=\"2px\" d=\"M4095,527.0 C4095,264.5 4585.0,264.5 4585.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-5c7977dc54f14ccf8d672621edb2a146-0-25\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M4585.0,529.0 L4593.0,517.0 4577.0,517.0\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-5c7977dc54f14ccf8d672621edb2a146-0-26\" stroke-width=\"2px\" d=\"M4795,527.0 C4795,439.5 4925.0,439.5 4925.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-5c7977dc54f14ccf8d672621edb2a146-0-26\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M4795,529.0 L4787,517.0 4803,517.0\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-5c7977dc54f14ccf8d672621edb2a146-0-27\" stroke-width=\"2px\" d=\"M4620,527.0 C4620,352.0 4930.0,352.0 4930.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-5c7977dc54f14ccf8d672621edb2a146-0-27\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">acl</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M4930.0,529.0 L4938.0,517.0 4922.0,517.0\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-5c7977dc54f14ccf8d672621edb2a146-0-28\" stroke-width=\"2px\" d=\"M4970,527.0 C4970,439.5 5100.0,439.5 5100.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-5c7977dc54f14ccf8d672621edb2a146-0-28\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M5100.0,529.0 L5108.0,517.0 5092.0,517.0\" fill=\"currentColor\"/>\n","</g>\n","</svg></span>"]},"metadata":{}}]},{"cell_type":"code","source":["with open(text_file) as f:\n","  text_data = f.read()"],"metadata":{"id":"aM1y74-yO_hy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load your usual SpaCy model (one of SpaCy English models)\n","import spacy\n","nlp = spacy.load('en_core_web_sm')\n","\n","# load NeuralCoref and add it to the pipe of SpaCy's model\n","import neuralcoref\n","coref = neuralcoref.NeuralCoref(nlp.vocab)\n","nlp.add_pipe(coref, name='neuralcoref')\n","\n","# You're done. You can now use NeuralCoref the same way you usually manipulate a SpaCy document and it's annotations."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"reCv9XfTPW4J","executionInfo":{"status":"ok","timestamp":1669076292969,"user_tz":300,"elapsed":2799,"user":{"displayName":"Catherine Mei","userId":"11274645310279339284"}},"outputId":"6174d647-25ac-4d64-fd18-a6924f9785b1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 40155833/40155833 [00:00<00:00, 48796122.80B/s]\n"]}]},{"cell_type":"markdown","source":["## Parsing the data\n","\n","Each line contains an English sentence and its corresponding Spanish sentence.\n","The English sentence is the *source sequence* and Spanish one is the *target sequence*.\n","We prepend the token `\"[start]\"` and we append the token `\"[end]\"` to the Spanish sentence."],"metadata":{"id":"1wwH_2yKSxHV"}},{"cell_type":"code","source":["feminine_markers = {'Ella', 'ella', 'Ellas', 'ellas', 'Mary', 'mary'}\n","masculine_markers = {'Él', 'él', 'Ellos', 'ellos', 'Tom', 'tom', 'Leo', 'leo'}"],"metadata":{"id":"Ela6kMH8s49U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open(text_file) as f:\n","    lines = f.read().split(\"\\n\")[:-1]"],"metadata":{"id":"7aziYAkjaRsX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text_pairs = []\n","\n","counter = 0\n","for line in lines:\n","    eng, spa = line.split(\"\\t\")\n","    spa_set = set(spa.split())\n","\n","    doc = nlp(eng)\n","    engList = eng.split()\n","    coreferences = doc._.coref_clusters\n","\n","    newEng = eng\n","\n","    corefLookup = {} # all objects -> original subject\n","\n","    for coref in coreferences:\n","      corefSubject, corefObjects = coref.mentions[0], set(coref.mentions[1:])\n","\n","      for obj in corefObjects:\n","        corefLookup[obj] = corefSubject\n","        newEng = newEng.replace(\" \" + str(obj) + \" \", \" [\" + str(corefSubject) + \"] \" + str(obj) + \" \")\n","        newEng = newEng.replace(\" \" + str(obj) + \".\", \" [\" + str(corefSubject) + \"] \" + str(obj) + \".\")\n","    \n","    adjEng = newEng\n","    for word in doc:\n","      auxWords = {\"is\", \"are\", \"was\", \"were\"}\n","      adjAlreadyAdded = set()\n","      if word.pos_ == \"NOUN\" or word.pos_ == \"PRON\":\n","        children = [child for child in word.children]\n","\n","        for child in children:\n","          if child.pos_ == \"ADJ\" and child.text not in adjAlreadyAdded:\n","            adjAlreadyAdded.add(child.text)\n","            adjObject = corefLookup[word.text] if word.text in corefLookup else word.text\n","            adjEng = adjEng.replace(\" \" + str(child.text) + \" \", \" [\" + str(adjObject) + \"] \" + str(child.text) + \" \")\n","            adjEng = adjEng.replace(\" \" + str(child.text) + \".\", \" [\" + str(adjObject) + \"] \" + str(child.text) + \".\")\n","\n","      elif word.text in auxWords:\n","        children = [child for child in word.children]\n","        \n","        adjective = []\n","        nounPronoun = []\n","\n","        for child in children:\n","          if child.pos_ == \"ADJ\":\n","            adjective.append(child)\n","          if child.pos_ == \"NOUN\" or child.pos_ == \"PRON\":\n","            nounPronoun.append(child)\n","\n","        if adjective and nounPronoun and adjective[0].text not in adjAlreadyAdded:\n","          adjAlreadyAdded.add(adjective[0].text)\n","          adjObject = corefLookup[nounPronoun[0].text] if nounPronoun[0].text in corefLookup else nounPronoun[0].text\n","          adjEng = adjEng.replace(\" \" + str(adjective[0].text) + \" \", \" [\" + str(adjObject) + \"] \" + str(adjective[0].text) + \" \")\n","          adjEng = adjEng.replace(\" \" + str(adjective[0].text) + \".\", \" [\" + str(adjObject) + \"] \" + str(adjective[0].text) + \".\")\n","\n","    eng = adjEng\n","\n","    spa = \"[start] \" + spa + \" [end]\"\n","    text_pairs.append((eng, spa))\n","    counter += 1\n","\n","    with open('coreferenceAdjectiveResultSpanish.txt', 'w') as g:\n","      g.write(eng + \"\\t\" + spa + \"\\n\")\n","\n","    if counter % 10000 == 0:\n","      print(\"Processed: \", counter)"],"metadata":{"id":"3MjylxTjSzQ-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669078922038,"user_tz":300,"elapsed":1521633,"user":{"displayName":"Catherine Mei","userId":"11274645310279339284"}},"outputId":"810a4483-12b5-4a78-a441-89890ee2e2c4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Processed:  10000\n","Processed:  20000\n","Processed:  30000\n","Processed:  40000\n","Processed:  50000\n","Processed:  60000\n","Processed:  70000\n","Processed:  80000\n","Processed:  90000\n","Processed:  100000\n","Processed:  110000\n"]}]},{"cell_type":"code","source":["# print(f\"Feminine ratio = {feminine_counter/(feminine_counter + masculine_counter)}\")"],"metadata":{"id":"7FwuEBJivZAI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Here's what our sentence pairs look like:"],"metadata":{"id":"Wb-z9nhlS_cc"}},{"cell_type":"code","source":["for _ in range(30):\n","    print(random.choice(text_pairs))"],"metadata":{"id":"n2LuQ-moTBxf","outputId":"7dd3348e-8c27-4ab2-c087-38727fc2c859","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669078965870,"user_tz":300,"elapsed":216,"user":{"displayName":"Catherine Mei","userId":"11274645310279339284"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["('Hand in your papers.', '[start] Entrega tus papeles. [end]')\n","(\"We can't leave any [ends] loose ends.\", '[start] No podemos dejar cabos sueltos. [end]')\n","(\"She's a [kisser] good kisser.\", '[start] Ella es buena besadora. [end]')\n","(\"I don't want you seeing Tom anymore.\", '[start] No quiero que veas más a Tom. [end]')\n","('I studied hard so that I could pass the examination.', '[start] Estudié mucho, de modo que pude aprobar el examen. [end]')\n","('He is a [champion] great swimming champion.', '[start] Él es un gran campeón de natación. [end]')\n","('Who will play the role of the princess?', '[start] ¿Quién hará el rol de la princesa? [end]')\n","('He broke the window on purpose.', '[start] Él rompió la ventana a propósito. [end]')\n","(\"Tom isn't interested either.\", '[start] Tom no está interesado tampoco. [end]')\n","('I found the diary that my father kept for 30 years.', '[start] Encontré el diario que mi padre mantuvo por 30 años. [end]')\n","('It is [It] better to live rich, than to die rich.', '[start] Es mejor vivir rico que morir rico. [end]')\n","('It is [It] clear that he is [he] rich.', '[start] Está claro que él es rico. [end]')\n","('Hey, Tom, open up.', '[start] Ey, Tom, abre. [end]')\n","(\"I've finished reading that book.\", '[start] Terminé de leer aquel libro. [end]')\n","(\"It's not a [question] difficult question.\", '[start] No es una pregunta difícil. [end]')\n","('Look at this.', '[start] Fíjate en esto. [end]')\n","(\"There's no need for violence.\", '[start] No hace falta la violencia. [end]')\n","('They found out.', '[start] Ellos lo descubrieron. [end]')\n","('They betrayed you.', '[start] Ellos te traicionaron. [end]')\n","('He got the job by a fluke.', '[start] Consiguió el trabajo por casualidad. [end]')\n","('I wish I had a million dollars.', '[start] Ojalá tuviera un millón de dólares. [end]')\n","('Father and I go fishing once in a while.', '[start] Yo y mi padre vamos de pesca de vez en cuando. [end]')\n","(\"I don't want to see your faces.\", '[start] No quiero veros las caras. [end]')\n","(\"It's incredible!\", '[start] ¡Increíble! [end]')\n","('The Greens are against everything.', '[start] Los verdes están en contra de todo. [end]')\n","('There were [accidents] several accidents caused by the fog.', '[start] Hubo varios accidentes motivados por la niebla. [end]')\n","('My house is [house] big.', '[start] Mi casa es grande. [end]')\n","('Do you have a book?', '[start] ¿Tiene usted un libro? [end]')\n","('I have a thorn in my finger.', '[start] Tengo una espina en el dedo. [end]')\n","('I called home.', '[start] Yo llamé a casa. [end]')\n"]}]},{"cell_type":"markdown","source":["Now, let's split the sentence pairs into a training set, a validation set,\n","and a test set."],"metadata":{"id":"xqaH7BVDTDI1"}},{"cell_type":"code","source":["random.shuffle(text_pairs)\n","num_val_samples = int(0.15 * len(text_pairs))\n","num_train_samples = len(text_pairs) - 2 * num_val_samples\n","train_pairs = text_pairs[:num_train_samples]\n","val_pairs = text_pairs[num_train_samples : num_train_samples + num_val_samples]\n","test_pairs = text_pairs[num_train_samples + num_val_samples :]\n","\n","print(f\"{len(text_pairs)} total pairs\")\n","print(f\"{len(train_pairs)} training pairs\")\n","print(f\"{len(val_pairs)} validation pairs\")\n","print(f\"{len(test_pairs)} test pairs\")"],"metadata":{"id":"jDJcO4gqTG-O","outputId":"cecffb00-853b-4631-9886-932884f2bbf6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669078968749,"user_tz":300,"elapsed":320,"user":{"displayName":"Catherine Mei","userId":"11274645310279339284"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["118964 total pairs\n","83276 training pairs\n","17844 validation pairs\n","17844 test pairs\n"]}]},{"cell_type":"markdown","source":["## Vectorizing the text data\n","\n","We'll use two instances of the `TextVectorization` layer to vectorize the text\n","data (one for English and one for Spanish),\n","that is to say, to turn the original strings into integer sequences\n","where each integer represents the index of a word in a vocabulary.\n","\n","The English layer will use the default string standardization (strip punctuation characters)\n","and splitting scheme (split on whitespace), while\n","the Spanish layer will use a custom standardization, where we add the character\n","`\"¿\"` to the set of punctuation characters to be stripped.\n","\n","Note: in a production-grade machine translation model, I would not recommend\n","stripping the punctuation characters in either language. Instead, I would recommend turning\n","each punctuation character into its own token,\n","which you could achieve by providing a custom `split` function to the `TextVectorization` layer."],"metadata":{"id":"fn7Ju9OOTLmp"}},{"cell_type":"code","source":["strip_chars = string.punctuation + \"¿\"\n","strip_chars = strip_chars.replace(\"[\", \"\")\n","strip_chars = strip_chars.replace(\"]\", \"\")\n","\n","vocab_size = 15000\n","sequence_length = 20\n","batch_size = 64\n","\n","\n","def custom_standardization(input_string):\n","    lowercase = tf.strings.lower(input_string)\n","    return tf.strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")\n","\n","\n","eng_vectorization = TextVectorization(\n","    max_tokens=vocab_size, output_mode=\"int\", output_sequence_length=sequence_length,\n",")\n","spa_vectorization = TextVectorization(\n","    max_tokens=vocab_size,\n","    output_mode=\"int\",\n","    output_sequence_length=sequence_length + 1,\n","    standardize=custom_standardization,\n",")\n","train_eng_texts = [pair[0] for pair in train_pairs]\n","train_spa_texts = [pair[1] for pair in train_pairs]\n","eng_vectorization.adapt(train_eng_texts)\n","spa_vectorization.adapt(train_spa_texts)"],"metadata":{"id":"op5NTCHXTNKQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Next, we'll format our datasets.\n","\n","At each training step, the model will seek to predict target words N+1 (and beyond)\n","using the source sentence and the target words 0 to N.\n","\n","As such, the training dataset will yield a tuple `(inputs, targets)`, where:\n","\n","- `inputs` is a dictionary with the keys `encoder_inputs` and `decoder_inputs`.\n","`encoder_inputs` is the vectorized source sentence and `encoder_inputs` is the target sentence \"so far\",\n","that is to say, the words 0 to N used to predict word N+1 (and beyond) in the target sentence.\n","- `target` is the target sentence offset by one step:\n","it provides the next words in the target sentence -- what the model will try to predict."],"metadata":{"id":"2VYzYa3DTZKw"}},{"cell_type":"code","source":["def format_dataset(eng, spa):\n","    eng = eng_vectorization(eng)\n","    spa = spa_vectorization(spa)\n","    return ({\"encoder_inputs\": eng, \"decoder_inputs\": spa[:, :-1],}, spa[:, 1:])\n","\n","\n","def make_dataset(pairs):\n","    eng_texts, spa_texts = zip(*pairs)\n","    eng_texts = list(eng_texts)\n","    spa_texts = list(spa_texts)\n","    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n","    dataset = dataset.batch(batch_size)\n","    dataset = dataset.map(format_dataset)\n","    return dataset.shuffle(2048).prefetch(16).cache()\n","\n","\n","train_ds = make_dataset(train_pairs)\n","val_ds = make_dataset(val_pairs)"],"metadata":{"id":"n_8ffCvTThbE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's take a quick look at the sequence shapes\n","(we have batches of 64 pairs, and all sequences are 20 steps long):"],"metadata":{"id":"3loHRGzyTtXT"}},{"cell_type":"markdown","source":[],"metadata":{"id":"DU0J8O6AT6jS"}},{"cell_type":"code","source":["for inputs, targets in train_ds.take(1):\n","    print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')\n","    print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')\n","    print(f\"targets.shape: {targets.shape}\")"],"metadata":{"id":"csEcLEYBT7M7","outputId":"e531a058-0d21-443f-f81e-d241340809d1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669078987986,"user_tz":300,"elapsed":751,"user":{"displayName":"Catherine Mei","userId":"11274645310279339284"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["inputs[\"encoder_inputs\"].shape: (64, 20)\n","inputs[\"decoder_inputs\"].shape: (64, 20)\n","targets.shape: (64, 20)\n"]}]},{"cell_type":"markdown","source":["## Building the model\n","\n","Our sequence-to-sequence Transformer consists of a `TransformerEncoder`\n","and a `TransformerDecoder` chained together. To make the model aware of word order,\n","we also use a `PositionalEmbedding` layer.\n","\n","The source sequence will be pass to the `TransformerEncoder`,\n","which will produce a new representation of it.\n","This new representation will then be passed\n","to the `TransformerDecoder`, together with the target sequence so far (target words 0 to N).\n","The `TransformerDecoder` will then seek to predict the next words in the target sequence (N+1 and beyond).\n","\n","A key detail that makes this possible is causal masking\n","(see method `get_causal_attention_mask()` on the `TransformerDecoder`).\n","The `TransformerDecoder` sees the entire sequences at once, and thus we must make\n","sure that it only uses information from target tokens 0 to N when predicting token N+1\n","(otherwise, it could use information from the future, which would\n","result in a model that cannot be used at inference time).\n"],"metadata":{"id":"olPbNVLEUBRw"}},{"cell_type":"code","source":["class TransformerEncoder(layers.Layer):\n","    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n","        super(TransformerEncoder, self).__init__(**kwargs)\n","        self.embed_dim = embed_dim\n","        self.dense_dim = dense_dim\n","        self.num_heads = num_heads\n","        self.attention = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=embed_dim\n","        )\n","        self.dense_proj = keras.Sequential(\n","            [layers.Dense(dense_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n","        )\n","        self.layernorm_1 = layers.LayerNormalization()\n","        self.layernorm_2 = layers.LayerNormalization()\n","        self.supports_masking = True\n","\n","    def call(self, inputs, mask=None):\n","        if mask is not None:\n","            padding_mask = tf.cast(mask[:, tf.newaxis, tf.newaxis, :], dtype=\"int32\")\n","        attention_output = self.attention(\n","            query=inputs, value=inputs, key=inputs, attention_mask=padding_mask\n","        )\n","        proj_input = self.layernorm_1(inputs + attention_output)\n","        proj_output = self.dense_proj(proj_input)\n","        return self.layernorm_2(proj_input + proj_output)\n","    def get_config(self):\n","        config = super().get_config()\n","        config.update({\n","            \"embed_dim\": self.embed_dim,\n","            \"dense_dim\": self.dense_dim,\n","            \"num_heads\": self.num_heads,\n","        })\n","        return config\n","\n","\n","class PositionalEmbedding(layers.Layer):\n","    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n","        super(PositionalEmbedding, self).__init__(**kwargs)\n","        self.token_embeddings = layers.Embedding(\n","            input_dim=vocab_size, output_dim=embed_dim\n","        )\n","        self.position_embeddings = layers.Embedding(\n","            input_dim=sequence_length, output_dim=embed_dim\n","        )\n","        self.sequence_length = sequence_length\n","        self.vocab_size = vocab_size\n","        self.embed_dim = embed_dim\n","\n","    def call(self, inputs):\n","        length = tf.shape(inputs)[-1]\n","        positions = tf.range(start=0, limit=length, delta=1)\n","        embedded_tokens = self.token_embeddings(inputs)\n","        embedded_positions = self.position_embeddings(positions)\n","        return embedded_tokens + embedded_positions\n","\n","    def compute_mask(self, inputs, mask=None):\n","        return tf.math.not_equal(inputs, 0)\n","    def get_config(self):\n","        config = super().get_config()\n","        config.update({\n","            \"sequence_length\": self.sequence_length,\n","            \"vocab_size\": self.vocab_size,\n","            \"embed_dim\": self.embed_dim,\n","        })\n","        return config\n","\n","\n","class TransformerDecoder(layers.Layer):\n","    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n","        super(TransformerDecoder, self).__init__(**kwargs)\n","        self.embed_dim = embed_dim\n","        self.latent_dim = latent_dim\n","        self.num_heads = num_heads\n","        self.attention_1 = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=embed_dim\n","        )\n","        self.attention_2 = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=embed_dim\n","        )\n","        self.dense_proj = keras.Sequential(\n","            [layers.Dense(latent_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n","        )\n","        self.layernorm_1 = layers.LayerNormalization()\n","        self.layernorm_2 = layers.LayerNormalization()\n","        self.layernorm_3 = layers.LayerNormalization()\n","        self.supports_masking = True\n","\n","    def call(self, inputs, encoder_outputs, mask=None):\n","        causal_mask = self.get_causal_attention_mask(inputs)\n","        if mask is not None:\n","            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n","            padding_mask = tf.minimum(padding_mask, causal_mask)\n","\n","        attention_output_1 = self.attention_1(\n","            query=inputs, value=inputs, key=inputs, attention_mask=causal_mask\n","        )\n","        out_1 = self.layernorm_1(inputs + attention_output_1)\n","\n","        attention_output_2 = self.attention_2(\n","            query=out_1,\n","            value=encoder_outputs,\n","            key=encoder_outputs,\n","            attention_mask=padding_mask,\n","        )\n","        out_2 = self.layernorm_2(out_1 + attention_output_2)\n","\n","        proj_output = self.dense_proj(out_2)\n","        return self.layernorm_3(out_2 + proj_output)\n","\n","    def get_causal_attention_mask(self, inputs):\n","        input_shape = tf.shape(inputs)\n","        batch_size, sequence_length = input_shape[0], input_shape[1]\n","        i = tf.range(sequence_length)[:, tf.newaxis]\n","        j = tf.range(sequence_length)\n","        mask = tf.cast(i >= j, dtype=\"int32\")\n","        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n","        mult = tf.concat(\n","            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n","            axis=0,\n","        )\n","        return tf.tile(mask, mult)\n","    def get_config(self):\n","        config = super().get_config()\n","        config.update({\n","            \"embed_dim\": self.embed_dim,\n","            \"latent_dim\": self.latent_dim,\n","            \"num_heads\": self.num_heads,\n","        })\n","        return config"],"metadata":{"id":"ZXa-hgFpULRp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["embed_dim = 256\n","latent_dim = 2048\n","num_heads = 8\n","\n","encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n","x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n","encoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(x)\n","encoder = keras.Model(encoder_inputs, encoder_outputs)\n","\n","decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n","encoded_seq_inputs = keras.Input(shape=(None, embed_dim), name=\"decoder_state_inputs\")\n","x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n","x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs)\n","x = layers.Dropout(0.5)(x)\n","decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n","decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n","\n","decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n","transformer = keras.Model(\n","    [encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\"\n",")"],"metadata":{"id":"iB1VrLfjUpnF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["epochs = 3  # This should be at least 30 for convergence\n","\n","transformer.summary()\n","transformer.compile(\n","    \"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",")\n","transformer.fit(train_ds, epochs=epochs, validation_data=val_ds)"],"metadata":{"id":"KYCElnuUUy0K","outputId":"0595d622-50fc-4b73-9fb3-879e57b521cb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669090283249,"user_tz":300,"elapsed":11293615,"user":{"displayName":"Catherine Mei","userId":"11274645310279339284"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"transformer\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," encoder_inputs (InputLayer)    [(None, None)]       0           []                               \n","                                                                                                  \n"," positional_embedding (Position  (None, None, 256)   3845120     ['encoder_inputs[0][0]']         \n"," alEmbedding)                                                                                     \n","                                                                                                  \n"," decoder_inputs (InputLayer)    [(None, None)]       0           []                               \n","                                                                                                  \n"," transformer_encoder (Transform  (None, None, 256)   3155456     ['positional_embedding[0][0]']   \n"," erEncoder)                                                                                       \n","                                                                                                  \n"," model_1 (Functional)           (None, None, 15000)  12959640    ['decoder_inputs[0][0]',         \n","                                                                  'transformer_encoder[0][0]']    \n","                                                                                                  \n","==================================================================================================\n","Total params: 19,960,216\n","Trainable params: 19,960,216\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","Epoch 1/3\n","1302/1302 [==============================] - 3816s 3s/step - loss: 1.6851 - accuracy: 0.4151 - val_loss: 1.3136 - val_accuracy: 0.5070\n","Epoch 2/3\n","1302/1302 [==============================] - 3731s 3s/step - loss: 1.3404 - accuracy: 0.5328 - val_loss: 1.1555 - val_accuracy: 0.5708\n","Epoch 3/3\n","1302/1302 [==============================] - 3736s 3s/step - loss: 1.1880 - accuracy: 0.5820 - val_loss: 1.0714 - val_accuracy: 0.6033\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f80a1a9a7d0>"]},"metadata":{},"execution_count":45}]},{"cell_type":"code","source":["spa_vocab = spa_vectorization.get_vocabulary()\n","spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n","max_decoded_sentence_length = 20\n","\n","\n","def decode_sequence(input_sentence):\n","    tokenized_input_sentence = eng_vectorization([input_sentence])\n","    decoded_sentence = \"[start]\"\n","    for i in range(max_decoded_sentence_length):\n","        tokenized_target_sentence = spa_vectorization([decoded_sentence])[:, :-1]\n","        predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n","\n","        sampled_token_index = np.argmax(predictions[0, i, :])\n","        sampled_token = spa_index_lookup[sampled_token_index]\n","        decoded_sentence += \" \" + sampled_token\n","\n","        if sampled_token == \"[end]\":\n","            break\n","    return decoded_sentence\n","\n","\n","test_eng_texts = [pair[0] for pair in test_pairs]\n","for _ in range(200):\n","    input_sentence = random.choice(test_eng_texts)\n","    translated = decode_sequence(input_sentence)\n","    print(input_sentence, translated)"],"metadata":{"id":"I2C4CPliU4zM","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d07f852a-f38d-4603-904d-26743a4d6c0f","executionInfo":{"status":"ok","timestamp":1669090377997,"user_tz":300,"elapsed":94756,"user":{"displayName":"Catherine Mei","userId":"11274645310279339284"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The thief ran away. [start] el fuego se [UNK] [end]\n","I've decided to do that by myself. [start] he hecho solo que me he hecho [end]\n","My wife loves apple pie. [start] a mi esposa le encanta la puerta [end]\n","Can I reserve a flight to Chicago? [start] puedo comprar un par de semanas [end]\n","She lays the table for breakfast. [start] ella está en la mesa [end]\n","I hope we achieve that. [start] espero que nos [UNK] eso [end]\n","Somebody took away my bag. [start] alguien se puso mi hija [end]\n","Did you miss me? [start] me has hecho [end]\n","The period is missing at the end of the sentence. [start] el tiempo se ha [UNK] el béisbol [end]\n","I'm thinking of going to the States during the summer vacation. [start] estoy pensando en el verano para los verano [end]\n","Tom asked Mary if [Tom] he could read [Mary] her mother's letter. [start] tom le preguntó a mary si pudo ser su madre [end]\n","You can't drink seawater because it is too [it] salty. [start] no puedes comer porque es tan bueno como es [end]\n","He had trouble breathing. [start] Él tenía problemas para hacer [end]\n","Is this fish still alive? [start] este es mi vivir todavía [end]\n","I'm grateful for what you've done for my family. [start] estoy [UNK] por lo que hizo a mi familia [end]\n","I really don't know much about that. [start] de verdad no le sabes mucho [end]\n","I remember that guy. [start] me siento ese tipo [end]\n","All had voted for Aaron Burr. [start] todo el mundo debería ser [UNK] [end]\n","She isn't [She] adequate to the task. [start] ella no es el trabajo para la tarde [end]\n","You know something. [start] tú sabes algo [end]\n","I'm not as rich as Tom. [start] no soy tan grande como tom [end]\n","We still have a lot to do. [start] todavía tenemos mucho que hacer [end]\n","Tom bet $300 on the fight. [start] tom se puso a la discusión [end]\n","I need a leather briefcase. [start] necesito un gran pañuelo [end]\n","I know someone who may be interested in your project. [start] sé que alguien puede ser tu que puede tu parte [end]\n","No one agreed with me. [start] nadie me [UNK] conmigo [end]\n","Why are you so quiet? [start] por qué eres tan popular [end]\n","We're friends. [start] somos amigos [end]\n","It is [It] true that he is over seventy. [start] es verdad que ella es [UNK] [end]\n","Could I have a piece of cheesecake? [start] podría tener un documento de mi vez [end]\n","I will put off my departure if it rains. [start] me voy a mi vez que se lo va a pasar [end]\n","I saw that film, but [that film] it wasn't very [it] good. [start] vi ese película pero no es tan bueno [end]\n","Nobody lives with me. [start] nadie vive conmigo [end]\n","I want to see him. [start] quiero ver [end]\n","Tom doesn't take [Tom] his job very seriously. [start] tom no se [UNK] su trabajo [end]\n","Please give me a coffee. [start] por favor [UNK] un café [end]\n","They talked on the phone every night. [start] ellos estaban en el teléfono de la noche [end]\n","My parents are very old [parents] fashioned. [start] mis padres son muy sentimientos [end]\n","This golf course is not [course] open to non-members. [start] este béisbol no es ni siquiera el béisbol para el final [end]\n","I suppose we could try to find Tom [we] ourselves. [start] creo que nos podría hacer a tom [end]\n","Tom decided to give up skateboarding after [Tom] his accident. [start] tom decidió dejar de [UNK] después de su accidente [end]\n","I didn't want to make any noise. [start] no quería hacer un ruido [end]\n","His lie complicated matters. [start] su idea es [UNK] [end]\n","He won't be a [player] good player. [start] Él no es un buen buen buen trabajo [end]\n","I wrote a book about China. [start] yo compré un libro de china [end]\n","I don't understand why you want it. [start] no entiendo por qué quieres [end]\n","Bring me that glass of milk. [start] [UNK] ese ese de poco de leche [end]\n","You could at least try to be a bit more polite, even though it's not like you. [start] puedes tener más que hacer un poco más que es más fácil como no te gusta [end]\n","Step back. [start] [UNK] [end]\n","Think of your family. [start] creo tu familia [end]\n","He smoked a cigar after lunch. [start] Él se dio un día de la hora [end]\n","I can't open the door. [start] no puedo ver la puerta [end]\n","I am not in the least concerned about the result. [start] no soy el menos al menos el río [end]\n","You have until midnight. [start] tienes que ir a la luz [end]\n","I left the window open all through the night. [start] me encontré la ventana por todos la noche [end]\n","It's a bit too complicated for me. [start] es un poco demasiado grande para mí [end]\n","Come over. [start] ven [end]\n","It was pretty [It] good. [start] era bastante bueno [end]\n","Tom has to protect [Tom] himself. [start] tom tiene que tener tom solo [end]\n","Tom is decorating [Tom] his room. [start] tom es su habitación [end]\n","They are very [They] old. [start] son muy muy viejo [end]\n","The weather report says [The weather report] it will rain tomorrow afternoon. [start] el tiempo se [UNK] el trabajo que se va a la tarde [end]\n","Tom is strict but fair. [start] tom es un estudiante más importante [end]\n","I was really [I] late. [start] era realmente tan tarde [end]\n","There's nothing [nothing] better than taking a [walk] nice walk. [start] no hay nada más que un poco que venir a dormir [end]\n","Go to sleep. [start] [UNK] a dormir [end]\n","We've got time. [start] tenemos tiempo [end]\n","My lips are sealed. [start] mis ojos están [UNK] [end]\n","Tom had nothing to do with that mess. [start] tom no tenía nada que hacer ese sofá [end]\n","It only took Mary a [minutes] few minutes to set the table. [start] solo se [UNK] a mary minutos para la mesa [end]\n","I would like to book a flight to Brazil. [start] me gustaría un libro para un libro [end]\n","I'd like to buy this doll. [start] me gustaría comprar esta oferta [end]\n","I don't have any feelings for Tom at all. [start] no tengo ningún par de llaves para tom [end]\n","Oslo is the capital of Norway. [start] [UNK] es la chica de [UNK] [end]\n","I'm going to see Mary this afternoon. [start] voy a ver esta casa esta tarde [end]\n","He hadn't eaten in two days. [start] Él no tenía dos días [end]\n","We'll pay for it. [start] [UNK] [end]\n","What is he up to? [start] qué es [UNK] [end]\n","Tom's novel has been translated into French. [start] la camisa de tom ha estado en el francés [end]\n","I spend most of my time at home. [start] yo haré la hora de mi casa [end]\n","All of my friends go to [schools] public schools. [start] todos mis amigos se va a ir a la escuela [end]\n","He heard a noise from the kitchen. [start] Él decidió un ruido de la cocina [end]\n","Why do you walk when you have a car? [start] por qué vas a conducir un coche [end]\n","Tom saw us. [start] tom nos vio [end]\n","Every time we go hiking, [Every time we go hiking] he falls behind. [start] todos los días nos [UNK] a la calle [end]\n","How do I get to the bookstore? [start] cómo se lo voy a la universidad [end]\n","Do all of you speak French? [start] todos te hablar francés [end]\n","I don't have time for you. [start] no tengo tiempo para ti [end]\n","That she was [she] tired was plain to see. [start] ella estaba cansado de ser cansado para ver [end]\n","I'm not tired at all. [start] no estoy cansado en todo [end]\n","Tom can speak French better than I can. [start] tom puede hablar más que yo yo [end]\n","How's the [feeling] patient feeling this morning? [start] cómo se ve a esta mañana [end]\n","Why do you not want to tell him about her? [start] por qué no quieres decir [end]\n","May I go to the movies tonight? [start] puedo ir al aeropuerto esta noche [end]\n","This is a book of children's stories. [start] esta es un libro de [UNK] [end]\n","Everyone wins. [start] todos los estudiantes [end]\n","You didn't finish filling out this form. [start] no te has hecho este sombrero [end]\n","Every year I take my family to the capital. [start] todos el año que mi familia a la chica [end]\n","Do you have a credit card? [start] tienes una oportunidad de conducir [end]\n","I'll try to find some way to do that. [start] voy a hacer una buena forma de hacer eso [end]\n","I plan to tell Tom that I don't want to go. [start] yo yo a decir tom no quería que quería ir [end]\n","The plane made a forced landing. [start] el avión de un ruido se [UNK] [end]\n","I don't eat the apple core. [start] no quiero la manzana [end]\n","She was dressed in black. [start] ella era un estudiante de mío [end]\n","Tom looks a bit tired. [start] tom se parece cansado [end]\n","First, we have to find out where they live. [start] el primera vez que tenemos dónde vive [end]\n","Don't make me do this. [start] no me lo haré [end]\n","The audience applauded the actress. [start] la [UNK] la manzana [end]\n","People love to check out each other's houses. [start] la gente le gusta el fútbol a los huevos [end]\n","Tom wept. [start] tom se [UNK] [end]\n","Monday's not good. [start] el dinero no es bueno [end]\n","Do you want me to be your bodyguard? [start] quieres que yo fuera tu casa [end]\n","The dog chased the rabbit. [start] el perro se [UNK] el rojo [end]\n","She kept me waiting for 30 minutes. [start] ella me hizo esperando minutos [end]\n","She doesn't live there any more. [start] ella no vive allí más [end]\n","I need to find a job. [start] necesito hacer un trabajo [end]\n","I've known you for years. [start] te he dicho años [end]\n","Tom has always been quiet. [start] tom siempre tiene frío [end]\n","They seem surprised. [start] ellos se parece [end]\n","The ladies were in evening dress. [start] la semana estaban en el vestido [end]\n","I'll do it when my turn comes. [start] lo haré cuando se va a la noche [end]\n","Keep me informed. [start] [UNK] [end]\n","She died in a plane crash. [start] ella murió en un accidente de avión [end]\n","Tell me about your plan. [start] dime tu tu plan [end]\n","Tom is unstoppable. [start] tom es [UNK] [end]\n","He raised [He] his hand to ask a question. [start] Él se dio la mano para una pregunta [end]\n","Do you want to sell them? [start] quieres [end]\n","Am I in trouble? [start] estoy en problemas [end]\n","We have to do something about what's going on. [start] tenemos que hacer algo lo que está haciendo [end]\n","Tom turned down the offer. [start] tom se puso la oferta [end]\n","Tom dozed. [start] tom se [UNK] [end]\n","I prefer coffee to tea. [start] prefiero el café [end]\n","The world is [world] full of fools. [start] el mundo está ocupado de la gente está [UNK] [end]\n","I will write to you [week] next week. [start] te voy a conducir el semana [end]\n","Watch him and do what [him] he does. [start] [UNK] y él lo que él es [end]\n","Please wait a [while] little while longer. [start] por favor [UNK] un poco más pronto [end]\n","I myself will repair that machine. [start] me voy a la oportunidad [end]\n","What would you like to do? [start] qué quieres hacer [end]\n","She fell asleep in my arms. [start] ella se sentó en mis reglas [end]\n","What does everyone do after school? [start] qué tiene todo el mundo después de la escuela [end]\n","Let me be happy. [start] deja que te [UNK] [end]\n","I know you're probably mad about what I said yesterday. [start] sé que estás seguro de lo que dije ayer [end]\n","Tom took the knife away from Mary. [start] tom se dio el único de mary [end]\n","I'm racking my brains to find a solution. [start] estoy mi tía para ver una oportunidad [end]\n","Tom scowled at Mary. [start] tom se [UNK] a mary [end]\n","I barely even remember Tom. [start] yo ni siquiera me importa tom [end]\n","I like your attitude. [start] me gusta tu tía [end]\n","I thought [it] it would be a [concert] good concert, but [it] it wasn't. [start] pensé que sea un buen concierto [end]\n","Her friends waited for [Her] her by the gate. [start] sus amigos ella ella [end]\n","I didn't see you. [start] no te vi [end]\n","Both were extremely rich. [start] los dos son rico [end]\n","Who is this girl? [start] quién es esta chica [end]\n","The boat anchored near the shore. [start] el edificio se puso [UNK] [end]\n","The Great Depression started in 1929 and ended in the 1930's. [start] la gran gran vino y se [UNK] en el camino [end]\n","My son can't count yet. [start] mi hijo todavía no puede [UNK] [end]\n","Everybody likes me. [start] a todos me gusta [end]\n","I've seen [days] better days. [start] he visto días [end]\n","It's snowing now. [start] ahora está en la noche [end]\n","This milk has a [taste] peculiar taste. [start] esta leche tiene un lápiz [end]\n","I'm not used to this kind of heat. [start] no soy tan grande de este tipo de calor [end]\n","Life begins when you are forty. [start] la vida cuando estás [UNK] [end]\n","Tom drinks. [start] tom habla [end]\n","A person is [person] worth more than money. [start] una persona es más más que dinero que el dinero [end]\n","Might I ask you a question? [start] te podría decir una pregunta [end]\n","You cheated. [start] tú eres [UNK] [end]\n","There is no need for you to stay here. [start] no hay que te [UNK] aquí [end]\n","What on earth are you doing here? [start] qué hora está haciendo aquí [end]\n","How's the weather there? [start] cómo está el tiempo [end]\n","Basketball is a lot of fun. [start] el es mucho de esto [end]\n","Something very [Something] strange happened in Salvador city. [start] algo muy interesante se [UNK] en la ciudad [end]\n","People have the tendency to speak more loudly when [People] they get excited. [start] la gente tiene que hablar más pronto cuando se [UNK] [end]\n","When did it happen? [start] cuándo hizo eso [end]\n","The violence lasted for two weeks. [start] la cerveza se [UNK] dos semanas [end]\n","What made you change your mind? [start] qué te hace tu salud [end]\n","My sisters were cooking when I came home. [start] mi hermana estaba tan pronto cuando estaba en casa [end]\n","Tom couldn't think of any reason for not giving Mary the present. [start] tom no pudo creer que no pudo decir la ayuda a mary [end]\n","I want to be on the [team] other team. [start] quiero ser en el equipo de equipo [end]\n","Plastic does not burn easily. [start] la verdad no te [UNK] [end]\n","Tom has trouble getting along with [students] other students in [Tom] his class. [start] tom tiene que estar bien el otro de su clase de clase [end]\n","My brothers and I all share a room. [start] mi hermano y todos se [UNK] a una habitación [end]\n","There is no need for you to stay here. [start] no hay que te [UNK] aquí [end]\n","I'm dying of thirst. [start] me estoy [UNK] [end]\n","I go to school. [start] voy a la escuela [end]\n","It was very [It] painful. [start] era muy grande [end]\n","Are you going to buy that car? [start] vas a que te [UNK] este coche [end]\n","You don't get up as early as your sister. [start] no te [UNK] tan pronto como tu hermana [end]\n","People are [People] funny. [start] la gente es [UNK] [end]\n","How [colors] many colors are there? [start] cuántos libros hay [end]\n","To make a [story] long story short, everything went fine. [start] para ser una historia tan grande todo lo que se va a hacer [end]\n","They're happy. [start] son feliz [end]\n","Sorry. I must've dialed the [number] wrong number. [start] lo siento que debe haber un problema [end]\n","He is [He] nasty. [start] Él es un día [end]\n","Years ago, she used to hang around with a bunch of bikers. [start] años hace una vez que se [UNK] a un poco de la de de la forma de [UNK] [end]\n","I want you to return to your seat. [start] quiero que te vaya a tu paraguas [end]\n","Tom's hiding something, too. [start] tom se está demasiado [end]\n","Tom enjoys reading novels. [start] tom se está de leer de leer [end]\n","You should fulfill your promises. [start] deberías [UNK] tu oferta [end]\n","Someone is standing behind the wall. [start] alguien está dormido [UNK] en la puerta [end]\n","The millionaire intended to purchase the masterpiece regardless of cost. [start] la hora se [UNK] el tiempo de [UNK] de los semanas [end]\n","I want to see him. [start] quiero ver [end]\n"]}]},{"cell_type":"code","source":["import numpy as np\n","from nltk.translate.bleu_score import sentence_bleu\n","translated_sentences = []\n","for i in range(100):\n","  input_sentence = test_eng_texts[i]\n","  translated = decode_sequence(input_sentence)\n","  translated_sentences.append(translated)\n","\n","test_sp_texts = [pair[1] for pair in test_pairs]\n","\n","bleu_scores = []\n","for j in range(len(translated_sentences)):\n","  bleu_scores.append(sentence_bleu(translated_sentences[j], test_sp_texts[j], weights = (1, 0, 0, 0)))\n","  print(translated_sentences[j], test_sp_texts[j] )\n","print(bleu_scores) \n","print(np.sum(np.array(bleu_scores))/len(bleu_scores))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ca-iNAAtdNSa","outputId":"ea86a505-9396-4f9d-b3a4-baaa863e8cb3","executionInfo":{"status":"ok","timestamp":1669090429863,"user_tz":300,"elapsed":51880,"user":{"displayName":"Catherine Mei","userId":"11274645310279339284"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n","The hypothesis contains 0 counts of 2-gram overlaps.\n","Therefore the BLEU score evaluates to 0, independently of\n","how many N-gram overlaps of lower order it contains.\n","Consider using lower n-gram order or use SmoothingFunction()\n","  warnings.warn(_msg)\n","/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n","The hypothesis contains 0 counts of 3-gram overlaps.\n","Therefore the BLEU score evaluates to 0, independently of\n","how many N-gram overlaps of lower order it contains.\n","Consider using lower n-gram order or use SmoothingFunction()\n","  warnings.warn(_msg)\n","/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n","The hypothesis contains 0 counts of 4-gram overlaps.\n","Therefore the BLEU score evaluates to 0, independently of\n","how many N-gram overlaps of lower order it contains.\n","Consider using lower n-gram order or use SmoothingFunction()\n","  warnings.warn(_msg)\n"]},{"output_type":"stream","name":"stdout","text":["[start] yo he ido a la guerra [end] [start] Yo estudié antes de la cena. [end]\n","[start] no te [UNK] [end] [start] No te encontrarán. [end]\n","[start] [UNK] a tu inglés [end] [start] Trata de mejorar tu inglés. [end]\n","[start] has oído el sofá [end] [start] ¿Viste el debate presidencial? [end]\n","[start] algunas personas se puede un poco de agua [end] [start] Algo de sal proviene de las minas, y algo del agua. [end]\n","[start] es la mismo tiempo [end] [start] Siempre es lo mismo. [end]\n","[start] el rojo está lejos de la calle [end] [start] Sudáfrica está lejos. [end]\n","[start] la reunión fue [UNK] [end] [start] La reunión fue cancelada. [end]\n","[start] no quiero que te [UNK] [end] [start] No quiero molestarte. [end]\n","[start] creo que es demasiado tarde para decir [end] [start] Supongo que es muy tarde para decir que no. [end]\n","[start] nadie vino a ayudar [end] [start] No vino nadie a ayudarme. [end]\n","[start] tom se puso anoche [end] [start] Tom se emborrachó anoche. [end]\n","[start] dónde está la oficina de dónde [end] [start] ¿Dónde está la oficina de correos? [end]\n","[start] tienen mucho tiempo [end] [start] Ellos tienen un montón de tiempo. [end]\n","[start] sé que alguien puede ser tu que puede tu parte [end] [start] Conozco a alguien que puede estar interesado en tu proyecto. [end]\n","[start] estás [UNK] [end] [start] Me estás embromando. [end]\n","[start] ella puede ser francés [end] [start] Ella podría ser francesa. [end]\n","[start] no quiero que te [UNK] [end] [start] No quiero asustarte. [end]\n","[start] por favor [UNK] este es [end] [start] Por favor, adopta este cachorrito. [end]\n","[start] pensé que tom tenía que fumar [end] [start] Pensé que Tom había dejado de fumar. [end]\n","[start] no te sé lo que quieres decir [end] [start] En realidad no entiendo lo que quieres decir. [end]\n","[start] por qué estás tan tan semanas [end] [start] ¿Por qué tienes tanta prisa? [end]\n","[start] cualquier vez es importante que [UNK] [end] [start] Cualquier comentario será agradecidamente apreciado. [end]\n","[start] cómo se puede decir el próximo próximo próximo [end] [start] ¿Cómo pueden prepararse los bancos para la próxima recesión? [end]\n","[start] la [UNK] de [UNK] su cabeza [end] [start] Los jabones aromáticos solían provocarle picor en la piel. [end]\n","[start] tienes un diccionario de el diccionario [end] [start] ¿Tenés un diccionario de francés? [end]\n","[start] su francés está un poco de poco [end] [start] Su francés mejora poco a poco. [end]\n","[start] las personas estaban [UNK] [end] [start] Afortunadamente, ningún pasajero resultó herido. [end]\n","[start] todos se [UNK] a un poco [UNK] [end] [start] Todos parecen sorprendidos y algo confusos. [end]\n","[start] tenemos un tiempo [end] [start] Lo pasamos de maravilla. [end]\n","[start] cuántos días te vas a hacer en australia [end] [start] ¿Cuántos días estarás en Londres? [end]\n","[start] voy a las diez [end] [start] Vendré como a las 10. [end]\n","[start] es un estudiante [end] [start] Se siente como un sueño. [end]\n","[start] quiero un resto [end] [start] Quiero descansar. [end]\n","[start] tom está seguro de que va a ir a tiempo [end] [start] Tom de seguro vendrá a tiempo. [end]\n","[start] me voy a la primera vez [end] [start] Yo mismo repararé esa máquina. [end]\n","[start] la [UNK] muy bien esta noche [end] [start] El tenor ha cantado muy bien esta noche. [end]\n","[start] esto es muy bueno [end] [start] Está muy bien. [end]\n","[start] tom tiene miedo de la vez [end] [start] Tom tiene miedo al compromiso. [end]\n","[start] tom se dice lo que había dicho [end] [start] Tom describió lo que había pasado. [end]\n","[start] no puedo decir decir [end] [start] No puedo decirlo con exactitud. [end]\n","[start] Él se [UNK] al otro de su salud es bueno [end] [start] Él nos sermoneó acerca de la importancia de una buena salud. [end]\n","[start] no va a la hora de que fuera [end] [start] No habrá una próxima vez. [end]\n","[start] me gustaría comprar esta oferta [end] [start] Me gustaría comprar esta muñeca. [end]\n","[start] nuestra lengua [end] [start] No digas palabrotas. [end]\n","[start] el tren fue [UNK] [end] [start] El tren descarriló. [end]\n","[start] [UNK] tus cosas [end] [start] Recoged vuestras cosas. [end]\n","[start] la chica se [UNK] en la puerta de [UNK] [end] [start] La niña usaba listones rojos en su pelo. [end]\n","[start] ellos [UNK] [end] [start] Van a regresar. [end]\n","[start] casi nunca me lo que le [UNK] [end] [start] Yo rara vez me encuentro con él. [end]\n","[start] tom le dijo a john que mary le dio a john a john como john a john por john [end] [start] Tom le dijo a John que él sacaría a Mary a bailar sólo si John sacaba a Alice. [end]\n","[start] tom es tres años más joven que mary [end] [start] Tom es tres años menor que Mary. [end]\n","[start] el [UNK] del examen se hizo difícil para el camino [end] [start] La inclinación del terreno hacía difícil la construcción de la carretera. [end]\n","[start] el teléfono de [UNK] [end] [start] El teléfono dejó de sonar. [end]\n","[start] pensé que tom se [UNK] conmigo [end] [start] Pensé que Tom estaría de acuerdo conmigo. [end]\n","[start] no quiero [end] [start] No quiero almorzar. [end]\n","[start] tienes que ayudar a tu trabajo [end] [start] Este trabajo hay que hacerlo con cabeza. [end]\n","[start] mi bicicleta me fue ayer [end] [start] Ayer me robaron la bicicleta. [end]\n","[start] tom siempre tiene el menos más pronto antes de la noche [end] [start] Tom siempre tiene hambre al menos una hora antes del almuerzo. [end]\n","[start] son [UNK] a tom [end] [start] Ellos están interrogando a Tom. [end]\n","[start] quiero una camisa de un perro [end] [start] Quiero una foto de ese perro. [end]\n","[start] si no había estado en el examen no me voy a la noche [end] [start] Si no hubiese sido por el cinturón de seguridad, yo no estaría vivo hoy. [end]\n","[start] creo que lo que te vaya si nos lo haré [end] [start] Si nos apuramos, creo que la hacemos. [end]\n","[start] solo quiero ser tu mejor que no es nada más [end] [start] Solo quiero ser tu amiga, nada más. [end]\n","[start] por qué no te [UNK] tom [end] [start] ¿Por qué no detuviste a Tom? [end]\n","[start] has estado alguna vez en nuestra hora [end] [start] ¿Has venido alguna vez a nuestras reuniones? [end]\n","[start] [UNK] a tom [end] [start] Sujeta a Tom. [end]\n","[start] el verano está en la escuela [end] [start] El desayuno se sirve a las siete. [end]\n","[start] estos son tom de un atrapado [end] [start] Estas son las botas de esquí de Tom. [end]\n","[start] algunas niños no le gusta los ojos [end] [start] A algunos niños no les gustan las verduras. [end]\n","[start] una persona [UNK] [UNK] para hacer cosas que son [UNK] [end] [start] Una persona ve las cosas de manera diferente dependiendo de si es rica o pobre. [end]\n","[start] no sé por qué estás tan tan bien [end] [start] No sé por qué están todos ustedes tan enojados. [end]\n","[start] mary se puso a la mano [end] [start] Mary se miró en el espejo. [end]\n","[start] nos hemos estado bien el día [end] [start] Ese día tuvimos un buen clima. [end]\n","[start] Él no va a trabajar tan pronto como solía [end] [start] Él no trabaja tanto como antes. [end]\n","[start] soy estudiante pero habla japonés [end] [start] Soy estadounidense, pero hablo un poco de japonés. [end]\n","[start] qué tienes en tu camisa [end] [start] ¿Qué tienes en tu bolso? [end]\n","[start] voy a casa a la casa [end] [start] Como muy tarde llegaré a casa a media noche. [end]\n","[start] mi esposa es un buen tiempo de acuerdo [end] [start] Mi abuelo es un poco duro de oído. [end]\n","[start] nos entiendo esto [end] [start] Entendemos esto. [end]\n","[start] la la vida [UNK] en un poco de [UNK] [end] [start] Las exportaciones de Japón superaron a las importaciones por $77.8 billones en el 1998. [end]\n","[start] todavía no estoy seguro [end] [start] Aún no estoy totalmente convencido. [end]\n","[start] Él nos dio un par de vacaciones de vacaciones [end] [start] Nos mandó escribir una redacción para las vacaciones. [end]\n","[start] tom le dio la comida [end] [start] Tom peló las zanahorias. [end]\n","[start] no tengo nadie al jugar [end] [start] No tengo a nadie con quien jugar. [end]\n","[start] no me siento lo que tengo que hacer [end] [start] No recuerdo lo que tengo que hacer. [end]\n","[start] hice a mary una manzana [end] [start] Yo hice una muñeca a Mary. [end]\n","[start] tom piensa que la comida es tan importante que la comida de la música [end] [start] Tom piensa que la comida que Mary prepara es demasiado picante. [end]\n","[start] le soy un doctor para un doctor [end] [start] Lo persuadí de consultar a un doctor. [end]\n","[start] hay un tiempo [end] [start] Pásenla bien. [end]\n","[start] eres un profesor [end] [start] Tú eres un profesor. [end]\n","[start] quería ser solo [end] [start] Yo quería estar sola. [end]\n","[start] me gustaría comprar este secreto [end] [start] Me gustaría comprar esta computadora. [end]\n","[start] sabía que no debería tener la tarde que me debería hacer el minuto [end] [start] Se que no debería haber pospuesto hacer los deberes hasta el último minuto. [end]\n","[start] ellos estaban [UNK] [end] [start] Ellos se comportaban extraño. [end]\n","[start] ella me habla en la vida [end] [start] Ella me habló en español. [end]\n","[start] cuándo te [UNK] francés [end] [start] ¿Cuándo empezaste a estudiar francés? [end]\n","[start] tú nuevo [end] [start] ¿Bebes vino? [end]\n","[start] tenemos mucho que hablar [end] [start] Tenemos mucho de que hablar. [end]\n","[start] te gusta el béisbol [end] [start] ¿Os gusta el béisbol? [end]\n","[0.3333333333333333, 0.375, 0.3658536585365854, 0.25, 0.23076923076923078, 0.4411764705882353, 0.42857142857142855, 0.38461538461538464, 0.42857142857142855, 0.2982456140350877, 0.38461538461538464, 0.3589743589743589, 0.3541666666666667, 0.31914893617021284, 0.21621621621621626, 0.32352941176470584, 0.3589743589743589, 0.4411764705882353, 0.2916666666666667, 0.3400000000000001, 0.2711864406779661, 0.3571428571428572, 0.2727272727272727, 0.25675675675675674, 0.18055555555555555, 0.2978723404255319, 0.36363636363636365, 0.20967741935483875, 0.24561403508771928, 0.3684210526315789, 0.2978723404255319, 0.34285714285714286, 0.3157894736842105, 0.41935483870967744, 0.4090909090909091, 0.3181818181818182, 0.3518518518518519, 0.5, 0.3181818181818182, 0.4166666666666667, 0.3333333333333333, 0.20270270270270271, 0.3589743589743589, 0.3695652173913043, 0.35294117647058826, 0.3333333333333333, 0.35135135135135137, 0.25925925925925924, 0.3448275862068966, 0.3260869565217391, 0.21739130434782608, 0.3478260869565218, 0.19540229885057472, 0.3499999999999999, 0.32727272727272727, 0.42424242424242425, 0.2962962962962963, 0.37209302325581395, 0.21052631578947367, 0.26666666666666666, 0.32558139534883723, 0.22093023255813954, 0.3333333333333333, 0.3469387755102041, 0.3571428571428572, 0.27586206896551724, 0.4444444444444444, 0.2978723404255319, 0.26, 0.2807017543859649, 0.15053763440860216, 0.26229508196721313, 0.375, 0.36363636363636365, 0.37777777777777777, 0.3125, 0.3421052631578948, 0.2413793103448276, 0.3333333333333333, 0.3666666666666667, 0.15841584158415845, 0.2653061224489796, 0.2537313432835821, 0.3684210526315789, 0.31914893617021284, 0.3469387755102041, 0.4, 0.23376623376623376, 0.29411764705882354, 0.4074074074074074, 0.4117647058823529, 0.42857142857142855, 0.3333333333333333, 0.22471910112359553, 0.3023255813953489, 0.3589743589743589, 0.3137254901960784, 0.46153846153846156, 0.42857142857142855, 0.4857142857142857]\n","0.3274266456109614\n"]}]},{"cell_type":"code","source":["for i in range(len(test_eng_texts)):\n","    input_sentence = test_eng_texts[i]\n","    translated = decode_sequence(input_sentence)\n","\n","    with open('translatedResult.txt', 'a') as h:\n","      h.write(input_sentence + \"\\t\" + translated + \"\\n\")"],"metadata":{"id":"UHmVoG0njZ2N"},"execution_count":null,"outputs":[]}]}