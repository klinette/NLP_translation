{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xKax1961_RY6","outputId":"effc21d3-e6f4-4283-c3ec-4ad733be904d","executionInfo":{"status":"ok","timestamp":1669952216081,"user_tz":300,"elapsed":6214,"user":{"displayName":"Linette Kunin","userId":"07994646427956674729"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["Cloning into 'neuralcoref'...\n","Checking out files:  98% (148/151)   \rChecking out files:  99% (150/151)   \rChecking out files: 100% (151/151)   \rChecking out files: 100% (151/151), done.\n"]}],"source":["%%bash\n","!(stat -t /usr/local/lib/*/dist-packages/google/colab > /dev/null 2>&1) && exit \n","git clone https://github.com/huggingface/neuralcoref.git"]},{"cell_type":"code","source":["# !git clone https://github.com/huggingface/neuralcoref.git\n","!pip install -U spacy\n","!python -m spacy download en_core_web_sm\n","\n","%cd neuralcoref\n","\n","!pip install -r requirements.txt\n","!pip install -e ."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8l2LVMF3R1YM","executionInfo":{"status":"ok","timestamp":1669952294907,"user_tz":300,"elapsed":77694,"user":{"displayName":"Linette Kunin","userId":"07994646427956674729"}},"outputId":"3692eed4-f16b-451c-8604-cfbf9e5c6d3a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: spacy in /usr/local/lib/python3.8/dist-packages (3.4.3)\n","Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.10.1)\n","Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.9.0)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.0.8)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (21.3)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.21.6)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.0.9)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.4.5)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (8.1.5)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy) (57.4.0)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.0.8)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (4.64.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.11.3)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.3.0)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.10.2)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.0.7)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.0.3)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.23.0)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.0.10)\n","Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.7.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->spacy) (3.0.9)\n","Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from pathy>=0.3.5->spacy) (5.2.1)\n","Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.1.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.9.24)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.3)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.9)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy) (7.1.2)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy) (2.0.1)\n","2022-12-02 03:37:22.674412: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting en-core-web-sm==3.4.1\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.1/en_core_web_sm-3.4.1-py3-none-any.whl (12.8 MB)\n","\u001b[K     |████████████████████████████████| 12.8 MB 30.1 MB/s \n","\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.8/dist-packages (from en-core-web-sm==3.4.1) (3.4.3)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.3.0)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.8)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.23.0)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.9)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.5)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (57.4.0)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.4.5)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.8)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.10)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.11.3)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.7)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.3)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.10.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (21.3)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.21.6)\n","Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\n","Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.9.0)\n","Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.64.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.9)\n","Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (5.2.1)\n","Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.1.1)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2022.9.24)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.10)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.0.3)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.9)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (7.1.2)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.1)\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_sm')\n","/content/neuralcoref\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting spacy<3.0.0,>=2.1.0\n","  Downloading spacy-2.3.8-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.0 MB)\n","\u001b[K     |████████████████████████████████| 5.0 MB 29.5 MB/s \n","\u001b[?25hRequirement already satisfied: cython>=0.25 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 2)) (0.29.32)\n","Requirement already satisfied: pytest in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 3)) (3.6.4)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (4.64.1)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (0.10.1)\n","Collecting catalogue<1.1.0,>=0.0.7\n","  Downloading catalogue-1.0.2-py2.py3-none-any.whl (16 kB)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (1.21.6)\n","Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (0.7.9)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (2.23.0)\n","Collecting thinc<7.5.0,>=7.4.1\n","  Downloading thinc-7.4.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n","\u001b[K     |████████████████████████████████| 1.1 MB 46.8 MB/s \n","\u001b[?25hRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (2.0.7)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (1.0.9)\n","Collecting plac<1.2.0,>=0.9.6\n","  Downloading plac-1.1.3-py2.py3-none-any.whl (20 kB)\n","Collecting srsly<1.1.0,>=1.0.2\n","  Downloading srsly-1.0.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (211 kB)\n","\u001b[K     |████████████████████████████████| 211 kB 55.6 MB/s \n","\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (57.4.0)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (3.0.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (2022.9.24)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (2.10)\n","Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from pytest->-r requirements.txt (line 3)) (9.0.0)\n","Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.8/dist-packages (from pytest->-r requirements.txt (line 3)) (1.4.1)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.8/dist-packages (from pytest->-r requirements.txt (line 3)) (1.15.0)\n","Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.8/dist-packages (from pytest->-r requirements.txt (line 3)) (0.7.1)\n","Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.8/dist-packages (from pytest->-r requirements.txt (line 3)) (22.1.0)\n","Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.8/dist-packages (from pytest->-r requirements.txt (line 3)) (1.11.0)\n","Installing collected packages: srsly, plac, catalogue, thinc, spacy\n","  Attempting uninstall: srsly\n","    Found existing installation: srsly 2.4.5\n","    Uninstalling srsly-2.4.5:\n","      Successfully uninstalled srsly-2.4.5\n","  Attempting uninstall: catalogue\n","    Found existing installation: catalogue 2.0.8\n","    Uninstalling catalogue-2.0.8:\n","      Successfully uninstalled catalogue-2.0.8\n","  Attempting uninstall: thinc\n","    Found existing installation: thinc 8.1.5\n","    Uninstalling thinc-8.1.5:\n","      Successfully uninstalled thinc-8.1.5\n","  Attempting uninstall: spacy\n","    Found existing installation: spacy 3.4.3\n","    Uninstalling spacy-3.4.3:\n","      Successfully uninstalled spacy-3.4.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","en-core-web-sm 3.4.1 requires spacy<3.5.0,>=3.4.0, but you have spacy 2.3.8 which is incompatible.\n","confection 0.0.3 requires srsly<3.0.0,>=2.4.0, but you have srsly 1.0.6 which is incompatible.\u001b[0m\n","Successfully installed catalogue-1.0.2 plac-1.1.3 spacy-2.3.8 srsly-1.0.6 thinc-7.4.6\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Obtaining file:///content/neuralcoref\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from neuralcoref==4.0) (1.21.6)\n","Collecting boto3\n","  Downloading boto3-1.26.21-py3-none-any.whl (132 kB)\n","\u001b[K     |████████████████████████████████| 132 kB 18.9 MB/s \n","\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from neuralcoref==4.0) (2.23.0)\n","Requirement already satisfied: spacy<3.0.0,>=2.1.0 in /usr/local/lib/python3.8/dist-packages (from neuralcoref==4.0) (2.3.8)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref==4.0) (2022.9.24)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref==4.0) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref==4.0) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref==4.0) (1.24.3)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (4.64.1)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (2.0.7)\n","Requirement already satisfied: thinc<7.5.0,>=7.4.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (7.4.6)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (3.0.8)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (1.0.9)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.8/dist-packages (from spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (1.0.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (57.4.0)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (0.10.1)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.8/dist-packages (from spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (1.1.3)\n","Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (0.7.9)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (1.0.6)\n","Collecting jmespath<2.0.0,>=0.7.1\n","  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n","Collecting s3transfer<0.7.0,>=0.6.0\n","  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n","\u001b[K     |████████████████████████████████| 79 kB 8.0 MB/s \n","\u001b[?25hCollecting botocore<1.30.0,>=1.29.21\n","  Downloading botocore-1.29.21-py3-none-any.whl (10.2 MB)\n","\u001b[K     |████████████████████████████████| 10.2 MB 54.2 MB/s \n","\u001b[?25hCollecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n","  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n","\u001b[K     |████████████████████████████████| 127 kB 63.3 MB/s \n","\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.8/dist-packages (from botocore<1.30.0,>=1.29.21->boto3->neuralcoref==4.0) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.21->boto3->neuralcoref==4.0) (1.15.0)\n","Installing collected packages: urllib3, jmespath, botocore, s3transfer, boto3, neuralcoref\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","  Running setup.py develop for neuralcoref\n","Successfully installed boto3-1.26.21 botocore-1.29.21 jmespath-1.0.1 neuralcoref-4.0 s3transfer-0.6.0 urllib3-1.25.11\n"]}]},{"cell_type":"code","source":["!python -m spacy download en_core_web_sm"],"metadata":{"id":"zkMFiXLLVKf-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669952339272,"user_tz":300,"elapsed":9343,"user":{"displayName":"Linette Kunin","userId":"07994646427956674729"}},"outputId":"7a78fdfe-636b-4c3e-9c19-f89675aab319"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting en_core_web_sm==2.3.1\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz (12.0 MB)\n","\u001b[K     |████████████████████████████████| 12.0 MB 24.6 MB/s \n","\u001b[?25hRequirement already satisfied: spacy<2.4.0,>=2.3.0 in /usr/local/lib/python3.8/dist-packages (from en_core_web_sm==2.3.1) (2.3.8)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.21.6)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.8/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.1.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (57.4.0)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.7)\n","Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.7.9)\n","Requirement already satisfied: thinc<7.5.0,>=7.4.1 in /usr/local/lib/python3.8/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (7.4.6)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.23.0)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.8)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.8/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.2)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.6)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.10.1)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.64.1)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.9)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.25.11)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2022.9.24)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.10)\n","Building wheels for collected packages: en-core-web-sm\n","  Building wheel for en-core-web-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for en-core-web-sm: filename=en_core_web_sm-2.3.1-py3-none-any.whl size=12047104 sha256=b1c2b57e30f3370f9e5a61d237b59881e0e8c310a047dcab4c74aba74b6e1ac6\n","  Stored in directory: /root/.cache/pip/wheels/ee/4d/f7/563214122be1540b5f9197b52cb3ddb9c4a8070808b22d5a84\n","Successfully built en-core-web-sm\n","Installing collected packages: en-core-web-sm\n","  Attempting uninstall: en-core-web-sm\n","    Found existing installation: en-core-web-sm 3.4.1\n","    Uninstalling en-core-web-sm-3.4.1:\n","      Successfully uninstalled en-core-web-sm-3.4.1\n","Successfully installed en-core-web-sm-2.3.1\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the model via spacy.load('en_core_web_sm')\n"]}]},{"cell_type":"code","source":["import pathlib\n","import random\n","import string\n","import re\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow.keras.layers import TextVectorization"],"metadata":{"id":"9ci9GuKxDGyO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Downloading the data\n","\n","We'll be working with an English-to-Spanish translation dataset\n","provided by [Anki](https://www.manythings.org/anki/). Let's download it:"],"metadata":{"id":"8vdoYt9gSsnH"}},{"cell_type":"code","source":["text_file = keras.utils.get_file(\n","    fname=\"spa-eng.zip\",\n","    origin=\"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\",\n","    extract=True,\n",")\n","text_file = pathlib.Path(text_file).parent / \"spa-eng\" / \"spa.txt\""],"metadata":{"id":"JaDJglFSStZL","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f20c506a-c785-49ab-e2c8-fed44c59a706","executionInfo":{"status":"ok","timestamp":1669952314502,"user_tz":300,"elapsed":301,"user":{"displayName":"Linette Kunin","userId":"07994646427956674729"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n","2638744/2638744 [==============================] - 0s 0us/step\n"]}]},{"cell_type":"code","source":["with open(text_file) as f:\n","  text_data = f.read()"],"metadata":{"id":"aM1y74-yO_hy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load your usual SpaCy model (one of SpaCy English models)\n","import spacy\n","nlp = spacy.load('en_core_web_sm')\n","\n","# load NeuralCoref and add it to the pipe of SpaCy's model\n","import neuralcoref\n","coref = neuralcoref.NeuralCoref(nlp.vocab)\n","nlp.add_pipe(coref, name='neuralcoref')\n","\n","# You're done. You can now use NeuralCoref the same way you usually manipulate a SpaCy document and it's annotations."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"reCv9XfTPW4J","executionInfo":{"status":"ok","timestamp":1669952346734,"user_tz":300,"elapsed":5129,"user":{"displayName":"Linette Kunin","userId":"07994646427956674729"}},"outputId":"524894e9-2389-4140-deb3-3f96ec8c55c0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 40155833/40155833 [00:01<00:00, 28299732.15B/s]\n"]}]},{"cell_type":"code","source":["doc._.coref_scores"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BWB640dYVjBk","executionInfo":{"status":"ok","timestamp":1669952358788,"user_tz":300,"elapsed":120,"user":{"displayName":"Linette Kunin","userId":"07994646427956674729"}},"outputId":"dd4de541-b648-4565-b65f-0c74803b967c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{My sister: {My sister: 1.3110305070877075},\n"," a dog: {a dog: 1.8047977685928345, My sister: -1.6718225479125977},\n"," She: {She: -0.09879469871520996,\n","  My sister: 8.039176940917969,\n","  a dog: -0.9113678932189941},\n"," him: {him: -1.3558179140090942,\n","  My sister: 2.681760549545288,\n","  a dog: 4.036096572875977,\n","  She: -3.2119688987731934},\n"," she: {she: -0.5818476676940918,\n","  My sister: 6.328548431396484,\n","  a dog: -0.9935951232910156,\n","  She: 8.811304092407227,\n","  him: -3.9658432006835938}}"]},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","source":["## Parsing the data\n","\n","Each line contains an English sentence and its corresponding Spanish sentence.\n","The English sentence is the *source sequence* and Spanish one is the *target sequence*.\n","We prepend the token `\"[start]\"` and we append the token `\"[end]\"` to the Spanish sentence."],"metadata":{"id":"1wwH_2yKSxHV"}},{"cell_type":"code","source":["feminine_markers = {'Ella', 'ella', 'Ellas', 'ellas', 'Mary', 'mary'}\n","masculine_markers = {'Él', 'él', 'Ellos', 'ellos', 'Tom', 'tom', 'Leo', 'leo'}"],"metadata":{"id":"Ela6kMH8s49U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open(text_file) as f:\n","    lines = f.read().split(\"\\n\")[:-1]"],"metadata":{"id":"7aziYAkjaRsX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text_pairs = []\n","\n","counter = 0\n","for line in lines:\n","    eng, spa = line.split(\"\\t\")\n","    spa_set = set(spa.split())\n","\n","    doc = nlp(eng)\n","    engList = eng.split()\n","    coreferences = doc._.coref_clusters\n","\n","    newEng = eng\n","\n","    for coref in coreferences:\n","      corefSubject, corefObjects = coref.mentions[0], set(coref.mentions[1:])\n","\n","      for obj in corefObjects:\n","        newEng = newEng.replace(\" \" + str(obj) + \" \", \" [\" + str(corefSubject) + \"] \" + str(obj) + \" \")\n","        newEng = newEng.replace(\" \" + str(obj) + \".\", \" [\" + str(corefSubject) + \"] \" + str(obj) + \".\")\n","    \n","    eng = newEng\n","\n","    spa = \"[start] \" + spa + \" [end]\"\n","    text_pairs.append((eng, spa))\n","    counter += 1\n","\n","    if counter % 10000 == 0:\n","      print(\"Processed: \", counter)"],"metadata":{"id":"3MjylxTjSzQ-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669953755017,"user_tz":300,"elapsed":1376732,"user":{"displayName":"Linette Kunin","userId":"07994646427956674729"}},"outputId":"6274a8b2-62a0-4523-bcbc-aeedc2d26071"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Processed:  10000\n","Processed:  20000\n","Processed:  30000\n","Processed:  40000\n","Processed:  50000\n","Processed:  60000\n","Processed:  70000\n","Processed:  80000\n","Processed:  90000\n","Processed:  100000\n","Processed:  110000\n"]}]},{"cell_type":"markdown","source":["Here's what our sentence pairs look like:"],"metadata":{"id":"Wb-z9nhlS_cc"}},{"cell_type":"code","source":["for _ in range(30):\n","    print(random.choice(text_pairs))"],"metadata":{"id":"n2LuQ-moTBxf","outputId":"150ed9a8-ff2b-4975-f1a6-2fbe4160a472","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669953874715,"user_tz":300,"elapsed":198,"user":{"displayName":"Linette Kunin","userId":"07994646427956674729"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(\"There's nothing worse than loneliness.\", '[start] No hay nada peor que la soledad. [end]')\n","('I have my doubts.', '[start] Tengo mis dudas. [end]')\n","(\"Let's forget it.\", '[start] Olvidémoslo. [end]')\n","('The mailman left a letter for [The mailman] her.', '[start] El cartero dejó una carta para ella. [end]')\n","('Dinner is ready, Father.', '[start] La cena está lista, papi. [end]')\n","('May I begin to eat?', '[start] ¿Puedo empezar a comer? [end]')\n","('Tom thought that [Tom] he was very lucky.', '[start] Tom pensó que era muy afortunado. [end]')\n","('She is accustomed to sitting.', '[start] Ella está acostumbrada a sentarse. [end]')\n","('Can you recommend a good play?', '[start] ¿Puedes recomendar una buena obra de teatro? [end]')\n","(\"That's my dress.\", '[start] Ese es mi vestido. [end]')\n","('I held up my hand to stop a taxi.', '[start] Levanté la mano para detener un taxi. [end]')\n","(\"Ah, that's much better.\", '[start] Ah, eso está mucho mejor. [end]')\n","('Tom liked Mary for years, but at some point, [Tom] his feelings for [Mary] her changed to love.', '[start] A Tom le había gustado Mary desde hacía varios años, pero, en algún momento, sus sentimientos hacia ella se convirtieron en amor. [end]')\n","(\"I only do what I'm paid to do.\", '[start] Solo hago lo que me pagan por hacer. [end]')\n","('This book is too difficult for me to understand.', '[start] Este libro es demasiado difícil para que lo entienda. [end]')\n","(\"Don't forget to put out the fire.\", '[start] No olvide apagar el fuego. [end]')\n","('Tom is very conceited.', '[start] Tom es muy arrogante. [end]')\n","('I need a crew.', '[start] Necesito una tripulación. [end]')\n","(\"Someone must've planted them.\", '[start] Alguien tiene que haberlos plantado. [end]')\n","(\"There's nowhere to hide.\", '[start] No hay donde esconderse. [end]')\n","(\"You can't park your car here.\", '[start] No puedes estacionar tu auto aquí. [end]')\n","('We are not Americans.', '[start] No somos americanos. [end]')\n","(\"If you'd told me about it earlier, I could've been free. However, tomorrow I have plans to go back home.\", '[start] Si me hubieras hablado antes al respecto, yo podría haber estado libre. Como sea, mañana tengo planes de volver a casa. [end]')\n","(\"I think that you're right.\", '[start] Creo que llevas razón. [end]')\n","(\"Tom didn't want to disappoint Mary.\", '[start] Tom no quiso decepcionar a Mary. [end]')\n","('She asked me to read 5 poems.', '[start] Me pidió que leyera 5 poemas. [end]')\n","(\"You're the champion, aren't you?\", '[start] Sos el campeón, ¿no? [end]')\n","('We have a lot of time.', '[start] Tenemos mucho tiempo. [end]')\n","('Has Tom ever been a problem?', '[start] ¿Alguna vez ha sido Tom un problema? [end]')\n","(\"It looks like it's quarter past ten now.\", '[start] Parece que ahora son las diez y cuarto. [end]')\n"]}]},{"cell_type":"markdown","source":["Now, let's split the sentence pairs into a training set, a validation set,\n","and a test set."],"metadata":{"id":"xqaH7BVDTDI1"}},{"cell_type":"code","source":["random.shuffle(text_pairs)\n","num_val_samples = int(0.15 * len(text_pairs))\n","num_train_samples = len(text_pairs) - 2 * num_val_samples\n","train_pairs = text_pairs[:num_train_samples]\n","val_pairs = text_pairs[num_train_samples : num_train_samples + num_val_samples]\n","test_pairs = text_pairs[num_train_samples + num_val_samples :]\n","\n","print(f\"{len(text_pairs)} total pairs\")\n","print(f\"{len(train_pairs)} training pairs\")\n","print(f\"{len(val_pairs)} validation pairs\")\n","print(f\"{len(test_pairs)} test pairs\")"],"metadata":{"id":"jDJcO4gqTG-O","outputId":"7882f354-55a6-41e5-965e-01bda90b9db8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669953877071,"user_tz":300,"elapsed":182,"user":{"displayName":"Linette Kunin","userId":"07994646427956674729"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["118964 total pairs\n","83276 training pairs\n","17844 validation pairs\n","17844 test pairs\n"]}]},{"cell_type":"markdown","source":["## Vectorizing the text data\n","\n","We'll use two instances of the `TextVectorization` layer to vectorize the text\n","data (one for English and one for Spanish),\n","that is to say, to turn the original strings into integer sequences\n","where each integer represents the index of a word in a vocabulary.\n","\n","The English layer will use the default string standardization (strip punctuation characters)\n","and splitting scheme (split on whitespace), while\n","the Spanish layer will use a custom standardization, where we add the character\n","`\"¿\"` to the set of punctuation characters to be stripped.\n","\n","Note: in a production-grade machine translation model, I would not recommend\n","stripping the punctuation characters in either language. Instead, I would recommend turning\n","each punctuation character into its own token,\n","which you could achieve by providing a custom `split` function to the `TextVectorization` layer."],"metadata":{"id":"fn7Ju9OOTLmp"}},{"cell_type":"code","source":["strip_chars = string.punctuation + \"¿\"\n","strip_chars = strip_chars.replace(\"[\", \"\")\n","strip_chars = strip_chars.replace(\"]\", \"\")\n","\n","vocab_size = 15000\n","sequence_length = 20\n","batch_size = 64\n","\n","\n","def custom_standardization(input_string):\n","    lowercase = tf.strings.lower(input_string)\n","    return tf.strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")\n","\n","\n","eng_vectorization = TextVectorization(\n","    max_tokens=vocab_size, output_mode=\"int\", output_sequence_length=sequence_length,\n",")\n","spa_vectorization = TextVectorization(\n","    max_tokens=vocab_size,\n","    output_mode=\"int\",\n","    output_sequence_length=sequence_length + 1,\n","    standardize=custom_standardization,\n",")\n","train_eng_texts = [pair[0] for pair in train_pairs]\n","train_spa_texts = [pair[1] for pair in train_pairs]\n","eng_vectorization.adapt(train_eng_texts)\n","spa_vectorization.adapt(train_spa_texts)"],"metadata":{"id":"op5NTCHXTNKQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Next, we'll format our datasets.\n","\n","At each training step, the model will seek to predict target words N+1 (and beyond)\n","using the source sentence and the target words 0 to N.\n","\n","As such, the training dataset will yield a tuple `(inputs, targets)`, where:\n","\n","- `inputs` is a dictionary with the keys `encoder_inputs` and `decoder_inputs`.\n","`encoder_inputs` is the vectorized source sentence and `encoder_inputs` is the target sentence \"so far\",\n","that is to say, the words 0 to N used to predict word N+1 (and beyond) in the target sentence.\n","- `target` is the target sentence offset by one step:\n","it provides the next words in the target sentence -- what the model will try to predict."],"metadata":{"id":"2VYzYa3DTZKw"}},{"cell_type":"code","source":["def format_dataset(eng, spa):\n","    eng = eng_vectorization(eng)\n","    spa = spa_vectorization(spa)\n","    return ({\"encoder_inputs\": eng, \"decoder_inputs\": spa[:, :-1],}, spa[:, 1:])\n","\n","\n","def make_dataset(pairs):\n","    eng_texts, spa_texts = zip(*pairs)\n","    eng_texts = list(eng_texts)\n","    spa_texts = list(spa_texts)\n","    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n","    dataset = dataset.batch(batch_size)\n","    dataset = dataset.map(format_dataset)\n","    return dataset.shuffle(2048).prefetch(16).cache()\n","\n","\n","train_ds = make_dataset(train_pairs)\n","val_ds = make_dataset(val_pairs)"],"metadata":{"id":"n_8ffCvTThbE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's take a quick look at the sequence shapes\n","(we have batches of 64 pairs, and all sequences are 20 steps long):"],"metadata":{"id":"3loHRGzyTtXT"}},{"cell_type":"markdown","source":[],"metadata":{"id":"DU0J8O6AT6jS"}},{"cell_type":"code","source":["for inputs, targets in train_ds.take(1):\n","    print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')\n","    print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')\n","    print(f\"targets.shape: {targets.shape}\")"],"metadata":{"id":"csEcLEYBT7M7","outputId":"89231b04-e8b0-4aab-9f88-34ace5dba296","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669953899087,"user_tz":300,"elapsed":931,"user":{"displayName":"Linette Kunin","userId":"07994646427956674729"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["inputs[\"encoder_inputs\"].shape: (64, 20)\n","inputs[\"decoder_inputs\"].shape: (64, 20)\n","targets.shape: (64, 20)\n"]}]},{"cell_type":"markdown","source":["## Building the model\n","\n","Our sequence-to-sequence Transformer consists of a `TransformerEncoder`\n","and a `TransformerDecoder` chained together. To make the model aware of word order,\n","we also use a `PositionalEmbedding` layer.\n","\n","The source sequence will be pass to the `TransformerEncoder`,\n","which will produce a new representation of it.\n","This new representation will then be passed\n","to the `TransformerDecoder`, together with the target sequence so far (target words 0 to N).\n","The `TransformerDecoder` will then seek to predict the next words in the target sequence (N+1 and beyond).\n","\n","A key detail that makes this possible is causal masking\n","(see method `get_causal_attention_mask()` on the `TransformerDecoder`).\n","The `TransformerDecoder` sees the entire sequences at once, and thus we must make\n","sure that it only uses information from target tokens 0 to N when predicting token N+1\n","(otherwise, it could use information from the future, which would\n","result in a model that cannot be used at inference time).\n"],"metadata":{"id":"olPbNVLEUBRw"}},{"cell_type":"code","source":["class TransformerEncoder(layers.Layer):\n","    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n","        super(TransformerEncoder, self).__init__(**kwargs)\n","        self.embed_dim = embed_dim\n","        self.dense_dim = dense_dim\n","        self.num_heads = num_heads\n","        self.attention = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=embed_dim\n","        )\n","        self.dense_proj = keras.Sequential(\n","            [layers.Dense(dense_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n","        )\n","        self.layernorm_1 = layers.LayerNormalization()\n","        self.layernorm_2 = layers.LayerNormalization()\n","        self.supports_masking = True\n","\n","    def call(self, inputs, mask=None):\n","        if mask is not None:\n","            padding_mask = tf.cast(mask[:, tf.newaxis, tf.newaxis, :], dtype=\"int32\")\n","        attention_output = self.attention(\n","            query=inputs, value=inputs, key=inputs, attention_mask=padding_mask\n","        )\n","        proj_input = self.layernorm_1(inputs + attention_output)\n","        proj_output = self.dense_proj(proj_input)\n","        return self.layernorm_2(proj_input + proj_output)\n","    def get_config(self):\n","        config = super().get_config()\n","        config.update({\n","            \"embed_dim\": self.embed_dim,\n","            \"dense_dim\": self.dense_dim,\n","            \"num_heads\": self.num_heads,\n","        })\n","        return config\n","\n","\n","class PositionalEmbedding(layers.Layer):\n","    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n","        super(PositionalEmbedding, self).__init__(**kwargs)\n","        self.token_embeddings = layers.Embedding(\n","            input_dim=vocab_size, output_dim=embed_dim\n","        )\n","        self.position_embeddings = layers.Embedding(\n","            input_dim=sequence_length, output_dim=embed_dim\n","        )\n","        self.sequence_length = sequence_length\n","        self.vocab_size = vocab_size\n","        self.embed_dim = embed_dim\n","\n","    def call(self, inputs):\n","        length = tf.shape(inputs)[-1]\n","        positions = tf.range(start=0, limit=length, delta=1)\n","        embedded_tokens = self.token_embeddings(inputs)\n","        embedded_positions = self.position_embeddings(positions)\n","        return embedded_tokens + embedded_positions\n","\n","    def compute_mask(self, inputs, mask=None):\n","        return tf.math.not_equal(inputs, 0)\n","    def get_config(self):\n","        config = super().get_config()\n","        config.update({\n","            \"sequence_length\": self.sequence_length,\n","            \"vocab_size\": self.vocab_size,\n","            \"embed_dim\": self.embed_dim,\n","        })\n","        return config\n","\n","\n","class TransformerDecoder(layers.Layer):\n","    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n","        super(TransformerDecoder, self).__init__(**kwargs)\n","        self.embed_dim = embed_dim\n","        self.latent_dim = latent_dim\n","        self.num_heads = num_heads\n","        self.attention_1 = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=embed_dim\n","        )\n","        self.attention_2 = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=embed_dim\n","        )\n","        self.dense_proj = keras.Sequential(\n","            [layers.Dense(latent_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n","        )\n","        self.layernorm_1 = layers.LayerNormalization()\n","        self.layernorm_2 = layers.LayerNormalization()\n","        self.layernorm_3 = layers.LayerNormalization()\n","        self.supports_masking = True\n","\n","    def call(self, inputs, encoder_outputs, mask=None):\n","        causal_mask = self.get_causal_attention_mask(inputs)\n","        if mask is not None:\n","            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n","            padding_mask = tf.minimum(padding_mask, causal_mask)\n","\n","        attention_output_1 = self.attention_1(\n","            query=inputs, value=inputs, key=inputs, attention_mask=causal_mask\n","        )\n","        out_1 = self.layernorm_1(inputs + attention_output_1)\n","\n","        attention_output_2 = self.attention_2(\n","            query=out_1,\n","            value=encoder_outputs,\n","            key=encoder_outputs,\n","            attention_mask=padding_mask,\n","        )\n","        out_2 = self.layernorm_2(out_1 + attention_output_2)\n","\n","        proj_output = self.dense_proj(out_2)\n","        return self.layernorm_3(out_2 + proj_output)\n","\n","    def get_causal_attention_mask(self, inputs):\n","        input_shape = tf.shape(inputs)\n","        batch_size, sequence_length = input_shape[0], input_shape[1]\n","        i = tf.range(sequence_length)[:, tf.newaxis]\n","        j = tf.range(sequence_length)\n","        mask = tf.cast(i >= j, dtype=\"int32\")\n","        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n","        mult = tf.concat(\n","            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n","            axis=0,\n","        )\n","        return tf.tile(mask, mult)\n","    def get_config(self):\n","        config = super().get_config()\n","        config.update({\n","            \"embed_dim\": self.embed_dim,\n","            \"latent_dim\": self.latent_dim,\n","            \"num_heads\": self.num_heads,\n","        })\n","        return config"],"metadata":{"id":"ZXa-hgFpULRp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["embed_dim = 256\n","latent_dim = 2048\n","num_heads = 8\n","\n","encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n","x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n","encoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(x)\n","encoder = keras.Model(encoder_inputs, encoder_outputs)\n","\n","decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n","encoded_seq_inputs = keras.Input(shape=(None, embed_dim), name=\"decoder_state_inputs\")\n","x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n","x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs)\n","x = layers.Dropout(0.5)(x)\n","decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n","decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n","\n","decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n","transformer = keras.Model(\n","    [encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\"\n",")"],"metadata":{"id":"iB1VrLfjUpnF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["epochs = 5  # This should be at least 30 for convergence\n","\n","transformer.summary()\n","transformer.compile(\n","    \"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",")\n","transformer.fit(train_ds, epochs=epochs, validation_data=val_ds)"],"metadata":{"id":"KYCElnuUUy0K","outputId":"2d0547f4-b0bd-4027-8ebc-4fe31e9da795","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669970732416,"user_tz":300,"elapsed":16818276,"user":{"displayName":"Linette Kunin","userId":"07994646427956674729"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"transformer\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," encoder_inputs (InputLayer)    [(None, None)]       0           []                               \n","                                                                                                  \n"," positional_embedding (Position  (None, None, 256)   3845120     ['encoder_inputs[0][0]']         \n"," alEmbedding)                                                                                     \n","                                                                                                  \n"," decoder_inputs (InputLayer)    [(None, None)]       0           []                               \n","                                                                                                  \n"," transformer_encoder (Transform  (None, None, 256)   3155456     ['positional_embedding[0][0]']   \n"," erEncoder)                                                                                       \n","                                                                                                  \n"," model_1 (Functional)           (None, None, 15000)  12959640    ['decoder_inputs[0][0]',         \n","                                                                  'transformer_encoder[0][0]']    \n","                                                                                                  \n","==================================================================================================\n","Total params: 19,960,216\n","Trainable params: 19,960,216\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","Epoch 1/5\n","1302/1302 [==============================] - 3362s 3s/step - loss: 1.6713 - accuracy: 0.4213 - val_loss: 1.3521 - val_accuracy: 0.5104\n","Epoch 2/5\n","1302/1302 [==============================] - 3343s 3s/step - loss: 1.3384 - accuracy: 0.5334 - val_loss: 1.1661 - val_accuracy: 0.5666\n","Epoch 3/5\n","1302/1302 [==============================] - 3344s 3s/step - loss: 1.1944 - accuracy: 0.5796 - val_loss: 1.0830 - val_accuracy: 0.5945\n","Epoch 4/5\n","1302/1302 [==============================] - 3356s 3s/step - loss: 1.1081 - accuracy: 0.6098 - val_loss: 1.0411 - val_accuracy: 0.6168\n","Epoch 5/5\n","1302/1302 [==============================] - 3363s 3s/step - loss: 1.0584 - accuracy: 0.6318 - val_loss: 1.0127 - val_accuracy: 0.6313\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f4870e88730>"]},"metadata":{},"execution_count":33}]},{"cell_type":"code","source":["spa_vocab = spa_vectorization.get_vocabulary()\n","spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n","max_decoded_sentence_length = 20\n","\n","\n","def decode_sequence(input_sentence):\n","    tokenized_input_sentence = eng_vectorization([input_sentence])\n","    decoded_sentence = \"[start]\"\n","    for i in range(max_decoded_sentence_length):\n","        tokenized_target_sentence = spa_vectorization([decoded_sentence])[:, :-1]\n","        predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n","\n","        sampled_token_index = np.argmax(predictions[0, i, :])\n","        sampled_token = spa_index_lookup[sampled_token_index]\n","        decoded_sentence += \" \" + sampled_token\n","\n","        if sampled_token == \"[end]\":\n","            break\n","    return decoded_sentence\n","\n","\n","test_eng_texts = [pair[0] for pair in test_pairs]\n","for _ in range(200):\n","    input_sentence = random.choice(test_eng_texts)\n","    translated = decode_sequence(input_sentence)\n","    print(input_sentence, translated)"],"metadata":{"id":"I2C4CPliU4zM","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d1809a22-209f-4608-966c-64b1e10803e3","executionInfo":{"status":"ok","timestamp":1669970829142,"user_tz":300,"elapsed":96742,"user":{"displayName":"Linette Kunin","userId":"07994646427956674729"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["We think the reason for his success was because of hard work. [start] creo que no [UNK] por su éxito [end]\n","My son is going to leave for France next week. [start] mi hijo va a ir a navidad en la semana [end]\n","These are simple sentences. [start] estos son [UNK] son [UNK] [end]\n","I'm looking forward to seeing you dance. [start] espero con ganas a ver quién te veo [end]\n","What is this about? [start] qué está de esto [end]\n","These are real. [start] estos son tu mía [end]\n","The teacher pointed [The teacher] [The teacher] her finger at [The teacher] me and asked [The teacher] me to go with [The teacher] [The teacher] her. [start] el profesor profesor a el profesor le [UNK] a él y me quería que le [UNK] a su profesor [end]\n","I didn't sell Tom my car. [start] no le he escrito a tom [end]\n","It is very cold today, isn't it? [start] hoy hace mucho frío [end]\n","What time does the bank open? [start] a qué hora del banco [UNK] [end]\n","Don't let go of my hand, or you'll get lost. [start] no te [UNK] mi mano o no se [UNK] [end]\n","Why is my dad in the kitchen? [start] por qué mi padre está en la cocina [end]\n","Where is your cap? [start] dónde está tu sueño [end]\n","Some words are hard to define. [start] algunas palabras son difícil de [UNK] [end]\n","We're pooped. [start] somos [UNK] [end]\n","What number bus do I take to get to Park Street? [start] qué número de inglés me voy a comprar la calle [end]\n","If you always eat that much, you'll gain weight. [start] si siempre [UNK] eso te [UNK] mucho [end]\n","Tom said [Tom] his house had been broken into. [start] tom dijo que su casa había estado en el había estado [end]\n","I'll see to it that you get to go. [start] te voy a ver que te vas a ir [end]\n","Tom and I swim together three times a week. [start] tom y yo dos veces juntos a una semana [end]\n","She died at the age of 54. [start] ella murió en la edad de hambre [end]\n","Tom can't speak French. [start] tom no puede hablar francés [end]\n","I was falsely accused. [start] yo estaba [UNK] [end]\n","I'm sorry. I know I shouldn't have done that. [start] lo siento no yo debería haber hecho eso [end]\n","She wanted me to come. [start] ella quería que él vino [end]\n","Don't criticize what you can't understand. [start] no te [UNK] lo que no lo [UNK] [end]\n","Did she sleep well? [start] ella se hizo bien [end]\n","She loves Tom, not me. [start] ella le encanta a mí [end]\n","They won't allow us to enter the garden. [start] no te [UNK] en el jardín [end]\n","I was asked to show my passport at the border. [start] le pedí a que le [UNK] el dedo [end]\n","I am from China. [start] soy de china [end]\n","Do you feel giving gifts is important? [start] tienes que [UNK] algo importante [end]\n","I usually go to bed at ten. [start] voy a la cama a las diez y a las diez [end]\n","A person with a BMI of 25 to 29 is considered overweight. [start] una persona con un [UNK] de [UNK] [UNK] [UNK] es [UNK] [end]\n","He ran away with the money. [start] Él salió con el dinero [end]\n","I hope you had a nice trip. [start] espero que te has tenido un viaje [end]\n","The sky brightened. [start] el sol se [UNK] [end]\n","Was it interesting? [start] fue interesante [end]\n","I only want to help. [start] solo quiero ayudar [end]\n","She washed the dishes and [She] she dried [the dishes] them. [start] ella se [UNK] a los gatos y los ojos [end]\n","What is the matter with your car? [start] cuál es el asunto de tu coche [end]\n","Tom says he's Canadian. [start] tom dice que es canadiense [end]\n","I'm looking forward to your reply. [start] espero con ganas a tu [UNK] [end]\n","If you want to go out with Tom, you can. [start] si quieres ir con tom [end]\n","He is full of energy. [start] está [UNK] de la de el tomás [end]\n","Tom doesn't have a fever. [start] tom no tiene un poco de hambre [end]\n","I'll get in touch with Tom by telephone tomorrow and ask [Tom] him to give [telephone] us a hand. [start] te voy a estudiar con tom y le [UNK] a él nos [UNK] la mano [end]\n","It seems likely. [start] parece ser verdad [end]\n","My aunt gave me an album. [start] mi madre me dio un extraño [end]\n","They can change. [start] ellos puede hablar [end]\n","I have a severe headache. [start] tengo un dolor de cabeza de cabeza [end]\n","The mother kissed [The mother] her baby. [start] la madre le encanta su hijo [end]\n","Tom was lucky. [start] tom fue suerte [end]\n","I'm trapped. [start] estoy [UNK] [end]\n","I have lost my keys. [start] he perdido mis llaves [end]\n","Why do the five yen coin and the fifty yen coin have holes in the center? [start] por qué las cinco de [UNK] y [UNK] en el [UNK] y [UNK] en el jardín [end]\n","Turn on your back. [start] [UNK] de tu espalda [end]\n","The rain hasn't stopped yet, has it? [start] la lluvia no ha hecho de él todavía [end]\n","Neither you nor I are mistaken. [start] no sé que está haciendo razón [end]\n","There was a sudden drop in the temperature last night. [start] había un [UNK] en la última vez que la última vez en la pena [end]\n","Tom wanted to fire Mary, but John stopped [Tom] him. [start] tom quería ser el fuego de mary pero lo había hecho [end]\n","It's the best we have. [start] es lo mejor que tenemos [end]\n","My brother is holding a camera in [My brother] his hand. [start] mi hermano está buscando una cámara en mi casa [end]\n","Can we postpone the trip? [start] podemos ser el viaje [end]\n","Why did you yell? [start] por qué le has hecho [end]\n","The doctor advised me to lose weight. [start] el médico me aconsejó que [UNK] mucho [end]\n","Tom ran to catch the train. [start] tom se [UNK] a [UNK] [end]\n","I used to work in Australia. [start] yo solía trabajar en australia [end]\n","God sent a sign. [start] qué [UNK] a un [UNK] de un [UNK] [end]\n","Have a good time on your date. [start] [UNK] bien [end]\n","You're very clever. [start] eres muy listo [end]\n","Tom does me a lot of favors. [start] tom me hace mucho mucho de tiempo [end]\n","I don't feel like singing now. [start] no tengo ganas de jugar a comer [end]\n","You should carry out your duty. [start] deberías tener la oferta con tu trabajo [end]\n","It was raining quietly. [start] estaba lloviendo a menudo [end]\n","You can set the white of an egg by boiling [an egg] it. [start] puedes [UNK] el sombrero es un [UNK] de [UNK] [end]\n","Everyone praises the boy. [start] todo el mundo [UNK] [end]\n","Minnesota's state bird is the mosquito. [start] el [UNK] del [UNK] es el [UNK] [end]\n","I don't feel like taking a walk now. [start] no tengo ganas de salir a comprar un poco [end]\n","We eat soup with a spoon. [start] nos gusta mucho con una [UNK] [end]\n","I'm glad to be invited to dinner. [start] me [UNK] que [UNK] a la cena [end]\n","Tom doesn't want to go to school. [start] tom no quiere ir a la escuela [end]\n","Where is he playing? [start] dónde está él [end]\n","Tom is grading papers. [start] tom está [UNK] de los [UNK] [end]\n","Pass me the salt, please. [start] por favor [UNK] la ropa [end]\n","I have to do it now. [start] ahora tengo que hacerlo [end]\n","It looks like Tom has an alibi for the night Mary was murdered. [start] parece que tom tiene una [UNK] para la noche que mary era que la que se había ido [end]\n","You were in a coma. [start] estabas en el [UNK] [end]\n","Please let me eat in peace. [start] por favor deja que comer en la televisión [end]\n","I need to go somewhere and think. [start] necesito ir a algún lugar en algún lugar [end]\n","I'm racking my brains to find a solution. [start] estoy [UNK] a mi [UNK] en una idea [end]\n","It's as smooth as a baby's bottom. [start] es tan [UNK] como un [UNK] de un [UNK] [end]\n","I received your letter. [start] he escrito tu carta [end]\n","It is wrong to steal money. [start] está mal mal dinero [end]\n","He has trouble waking up on time. [start] Él tiene problemas para [UNK] de tiempo [end]\n","I punched him in the face. [start] le [UNK] en su cara [end]\n","I never thought it'd be this hard to build a picnic table. [start] nunca pensé que sería tan difícil ser de una mesa [end]\n","If [he] he had known [her] her phone number, [he] he could have called [her] her up. [start] si él le había oído su teléfono podía [UNK] [end]\n","They say Tom is gone. [start] ellos digas que tom se ha ido [end]\n","I'll get there in an hour. [start] voy a estar en una hora [end]\n","How's your married life? [start] cómo está tu vida en la vida [end]\n","I'm never going to get married. [start] nunca voy a ir a casado [end]\n","He's a little tipsy. [start] es un poco de poco de [UNK] [end]\n","I need to buy food, but I don't have enough money. [start] necesito comprar comida pero no tengo suficiente dinero [end]\n","I bought a Nintendo 3DS. [start] compré un [UNK] [UNK] [end]\n","Thunder indicates that a storm is near. [start] el [UNK] que una tormenta está cerca de una tormenta [end]\n","I am not always at home on Sundays. [start] no estoy siempre en casa en la domingos [end]\n","I know you'll like Tom. [start] sé que te gusta tom [end]\n","There's a huge hole in the wall. [start] hay un gran [UNK] en la vida [end]\n","In case of an emergency, phone me at this number. [start] en este taxi me [UNK] su número de número [end]\n","It's a really good book. [start] es muy bueno o no [end]\n","There are many islands in Greece. [start] hay muchos [UNK] en [UNK] [end]\n","Tom will always be here for [Tom] you. [start] tom siempre va a estar aquí para ti [end]\n","They're the same age. [start] son los mismo de la misma edad [end]\n","I know you're right. [start] sé que tienes razón [end]\n","I wasn't on time for school this morning. [start] no estaba en la mañana en tiempo [end]\n","Tom is eating oysters. [start] tom está comer bien [end]\n","He will be free tomorrow. [start] mañana será libre [end]\n","These shoes are very comfortable. [start] estos zapatos son muy [UNK] [end]\n","I love the smell of pancakes in the morning. [start] me encanta el [UNK] de la mañana en el [UNK] [end]\n","He has a sufficient income to support [He] his family. [start] Él tiene un [UNK] de [UNK] a su familia [end]\n","Tom couldn't eat the whole sandwich. [start] tom no podía comer nada de un trabajo [end]\n","I know what you're going to say. [start] sé lo que vas a decir [end]\n","That boy talks as if [That boy] he were a grown up. [start] ese niño habla como si fuera un niño [end]\n","Tom is single. [start] tom es [UNK] [end]\n","The day when we first met was a rainy day. [start] el día que nos [UNK] en un día fue un día [end]\n","What were we talking about? [start] de qué estabas hablando [end]\n","He always leaves the window open while [He] he sleeps. [start] Él siempre va a la ventana mientras él se [UNK] [end]\n","Tom claimed [Tom] he had been captured by the enemy. [start] tom [UNK] que había estado en [UNK] por el accidente [end]\n","I don't think it's necessary for us to do that. [start] no creo que es imposible que lo [UNK] [end]\n","Is there a letter for me? [start] hay una carta en mí [end]\n","Tom is running a little late. [start] tom está [UNK] un poco tarde [end]\n","Please call your first witness. [start] por favor [UNK] tu primera vez [end]\n","We're distancing [We] ourselves from the rest of the family. [start] nos [UNK] con el plan de la familia [end]\n","Learn to keep time. [start] [UNK] a tiempo [end]\n","Why don't you just believe me? [start] por qué no me [UNK] [end]\n","It's very slow. [start] es muy [UNK] [end]\n","Is this all for real? [start] esto es todo por su suerte [end]\n","He lied about it. [start] Él le [UNK] [end]\n","Tom is snoring. [start] tom está [UNK] [end]\n","How long have you been in Kobe? [start] cuánto tiempo has estado en [UNK] [end]\n","Do you promise? [start] te [UNK] [end]\n","I have a flexible schedule. [start] tengo una [UNK] de un [UNK] [end]\n","Tom reloaded [Tom] his gun. [start] tom le [UNK] su suerte [end]\n","Let's get you fixed up with a drink. [start] te vamos a [UNK] una [UNK] [end]\n","Why are you guys so angry? [start] por qué estás tan tan [UNK] [end]\n","How long did they live in England? [start] cuánto tiempo viven en tokio [end]\n","What is his name? [start] cuál es su nombre [end]\n","Mary wanted to marry a man with ambition. [start] mary quería ser con un hombre [UNK] con su [UNK] [end]\n","His neighbors are suspicious of [His] him. [start] sus [UNK] son [UNK] de él [end]\n","Have you ever failed an exam? [start] alguna vez has ido a un examen [end]\n","I can understand what she is saying. [start] puedo entender lo que ella está diciendo [end]\n","He claims that [He] he is honest. [start] Él es que él es [UNK] [end]\n","Go get her medicine and a glass of water. [start] ve como un [UNK] y un agua de agua [end]\n","Pleased to meet you. [start] te gusta verte [end]\n","I'm feeling claustrophobic. [start] me estoy [UNK] [end]\n","You have to use the money wisely. [start] tienes que usar el dinero de la dinero [end]\n","She follows [She] her brother wherever [her brother] he goes. [start] ella le [UNK] a su hermano que le va [end]\n","Take whichever one you like. [start] [UNK] que te gusta [end]\n","I recommend you to go by train. [start] te espero que vas a ir a tren [end]\n","Time seemed to stop. [start] la vez parecía de dejar [end]\n","You know what you must do. [start] sabes lo que tienes que hacer [end]\n","Tom has to be careful about what [Tom] he eats. [start] tom tiene que ser cuidado de lo que él había pasado [end]\n","I'll do what you ask. [start] te voy a decir [end]\n","Someone is watching you. [start] alguien te está buscando [end]\n","Did you notice both pictures are very similar? [start] te [UNK] dos gustan las fotos muy [UNK] [end]\n","You don't realize its value until you have lost your health. [start] no te [UNK] tanto que te has hecho tu salud [end]\n","I wish that I could speak French as well as Tom. [start] ojalá no podía hablar francés en francés tan bien como tom [end]\n","You may be right. [start] puede ser razón [end]\n","Sirius is one of the 27 stars on the flag of Brazil. [start] el [UNK] es una de las [UNK] de las [UNK] de la de tom [end]\n","You need to wear a tuxedo. [start] necesitas usar un [UNK] [end]\n","It's hard to choose. [start] es difícil de escribir [end]\n","He enjoyed playing baseball. [start] Él estaba jugando al béisbol [end]\n","Drinking is harmful for your health. [start] [UNK] es bueno para la salud [end]\n","I went out with my friend. [start] fui con mi amigo [end]\n","The Jacksons are here. [start] los [UNK] están aquí [end]\n","An immense monument was erected in honor of the noble patriot. [start] se [UNK] un [UNK] de [UNK] en la [UNK] de [UNK] [end]\n","Tom is much older than Mary. [start] tom es mucho mayor que mary [end]\n","Why did you decide to name your son Tom? [start] por qué te has hecho tu hijo [end]\n","I never heard from him again. [start] nunca la oído de nuevo [end]\n","The rebels took control of the capital. [start] el [UNK] se [UNK] de la semana [end]\n","They were leaving Japan the next day. [start] ellos nos [UNK] el año que viene [end]\n","I do not have any money. [start] no tengo un poco de dinero [end]\n","Everyone remained calm. [start] todo el mundo estaba [UNK] [end]\n","Did you see grandpa's wheelchair? [start] has visto la tienda de la ropa de [UNK] [end]\n","The toilet won't stop running. [start] la [UNK] no se [UNK] [end]\n","She's unconscious. [start] ella está [UNK] [end]\n","Unfortunately, we didn't find it out in time. [start] [UNK] no lo [UNK] en tiempo [end]\n","I had Tom do it instead. [start] yo tuve que tom en tiempo [end]\n","We started singing. [start] nosotros [UNK] a cantar [end]\n","Tom was tempted to tell Mary the truth. [start] tom estaba [UNK] a decir la verdad [end]\n","I hope to hear from you soon. [start] espero hablar de pronto te [UNK] [end]\n","I was woken up by the sound of thunder this morning. [start] estaba [UNK] por la [UNK] del jardín esta mañana [end]\n","There will be consequences. [start] allí será las [UNK] [end]\n","Mary's husband is rich. [start] el profesor de mary es rico [end]\n","Have you asked the others? [start] te has preguntó a los otros [end]\n","I have two older sisters. [start] tengo dos veces mayor y hermana [end]\n","Let's see what happens. [start] vamos a ver a qué pasa [end]\n","We were very sad. [start] estaban muy [UNK] [end]\n","What are they talking about? [start] de qué están hablando [end]\n"]}]},{"cell_type":"code","source":["import numpy as np\n","from nltk.translate.bleu_score import sentence_bleu\n","translated_sentences = []\n","for i in range(100):\n","  input_sentence = test_eng_texts[i]\n","  translated = decode_sequence(input_sentence)\n","  translated_sentences.append(translated)\n","\n","test_sp_texts = [pair[1] for pair in test_pairs]\n","\n","bleu_scores = []\n","for j in range(len(translated_sentences)):\n","  bleu_scores.append(sentence_bleu(translated_sentences[j], test_sp_texts[j], weights = (1, 0, 0, 0)))\n","  print(translated_sentences[j], test_sp_texts[j] )\n","print(bleu_scores) \n","print(np.sum(np.array(bleu_scores))/len(bleu_scores))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ca-iNAAtdNSa","outputId":"a8dd996e-d119-41f7-9f65-0f1c679509d3","executionInfo":{"status":"ok","timestamp":1669970877888,"user_tz":300,"elapsed":48762,"user":{"displayName":"Linette Kunin","userId":"07994646427956674729"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n","The hypothesis contains 0 counts of 2-gram overlaps.\n","Therefore the BLEU score evaluates to 0, independently of\n","how many N-gram overlaps of lower order it contains.\n","Consider using lower n-gram order or use SmoothingFunction()\n","  warnings.warn(_msg)\n","/usr/local/lib/python3.8/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n","The hypothesis contains 0 counts of 3-gram overlaps.\n","Therefore the BLEU score evaluates to 0, independently of\n","how many N-gram overlaps of lower order it contains.\n","Consider using lower n-gram order or use SmoothingFunction()\n","  warnings.warn(_msg)\n","/usr/local/lib/python3.8/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n","The hypothesis contains 0 counts of 4-gram overlaps.\n","Therefore the BLEU score evaluates to 0, independently of\n","how many N-gram overlaps of lower order it contains.\n","Consider using lower n-gram order or use SmoothingFunction()\n","  warnings.warn(_msg)\n"]},{"output_type":"stream","name":"stdout","text":["[start] quieres mi ayuda [end] [start] ¿Queréis mi ayuda? [end]\n","[start] yo me he buscando un hombre ahora [end] [start] Me acabo de encontrar a tu padre. [end]\n","[start] [UNK] un poco [end] [start] Tomémonos algo. [end]\n","[start] están [UNK] y [UNK] [end] [start] Están hechos un manojo de nervios. [end]\n","[start] ella está intentando hacer pero no sabe saber cómo ella [end] [start] Está intentando silbar, pero no sabe cómo. [end]\n","[start] Él no se [UNK] de esa cuenta que no tiene tiempo para comer todas las cosas [end] [start] No se da cuenta de que no tiene tiempo para estas cosas. [end]\n","[start] lo ellos lo hizo más rápido que nosotros [end] [start] Ellos lo hacen más rápido que nosotros. [end]\n","[start] ese tipo tiene mucho de [UNK] [end] [start] Ese tipo es muy fresco. [end]\n","[start] no tienes que ser [UNK] [end] [start] No tienes por qué avergonzarte. [end]\n","[start] bueno trabajo tom [end] [start] Buen trabajo, Tom. [end]\n","[start] no puedo [UNK] [end] [start] No puedo levantarme. [end]\n","[start] tienes que dejar [end] [start] Tienes que parar. [end]\n","[start] si quieres una guitarra quieres que te [UNK] [end] [start] Si quieres calidad, paga por ella. [end]\n","[start] esto es muy [UNK] [end] [start] Esto es muy escalofriante. [end]\n","[start] no sé es de mi éxito [end] [start] Sé que no es asunto mío. [end]\n","[start] [UNK] tan pronto como tú [end] [start] Salta lo más alto que puedas. [end]\n","[start] Él era tan rico que no podía dormir [end] [start] Era tan pobre, que no podía comprar pan. [end]\n","[start] tengo seis seis de [UNK] [end] [start] Mido un metro y ochenta y tres centímetros de altura. [end]\n","[start] tom quiere que te [UNK] en boston [end] [start] Tomás quiere que me mude a Boston. [end]\n","[start] tom no va al trabajo que trabajar [end] [start] Tom no va al trabajo el domingo. [end]\n","[start] te voy a ti [end] [start] Te demandaré. [end]\n","[start] tu [UNK] es extraño del mío [end] [start] Tu forma de hacerlo es diferente a la mía. [end]\n","[start] deja hablar con tom solo [end] [start] Déjame hablar con Tom a solas. [end]\n","[start] tom parecía como si la vi un [UNK] [end] [start] Tom lucía como si hubiera visto un fantasma. [end]\n","[start] lo haré otra vez [end] [start] Lo intentaré otra vez. [end]\n","[start] qué te [UNK] [end] [start] ¿Con qué te golpearon? [end]\n","[start] su jardín es trabajo de béisbol [end] [start] Su jardín es una obra de arte. [end]\n","[start] no te quiero que te vas a [UNK] [end] [start] No quiero que te preocupes. [end]\n","[start] no te [UNK] tanto que te has hecho tu salud [end] [start] No es sino hasta que pierdes la salud que te das cuenta de su valor. [end]\n","[start] cómo puedo hacerlo [end] [start] Mire cómo lo hago yo. [end]\n","[start] te gustaría un poco de leche [end] [start] ¿Quieres ensalada? [end]\n","[start] mi profesor dice que debería ser más [UNK] [end] [start] Mi profesor de la autoescuela dice que debería tener más paciencia. [end]\n","[start] te [UNK] tu coche por favor [end] [start] ¿Podría usted mover su coche, por favor? [end]\n","[start] me estoy muy bien feliz [end] [start] Me alegra que lo haga feliz. [end]\n","[start] no me gusta vivir en el país [end] [start] No me gusta vivir en el campo. [end]\n","[start] Él está haciendo mejor [end] [start] Él está mejorando. [end]\n","[start] no estás buscando mi pregunta [end] [start] No estás respondiendo a mi pregunta. [end]\n","[start] no estoy en contra de la gente de contra de la [UNK] [end] [start] No estoy en contra de la ilegalización de las armas. [end]\n","[start] tenemos que encontrar un lugar lugar para [UNK] [end] [start] Necesitamos encontrar un sitio seguro para escondernos. [end]\n","[start] tom quería decir algo con mary [end] [start] Tom quería decirle algo a Mary. [end]\n","[start] cuánto tiempo se va a ir a ir a tren [end] [start] ¿Cuánto se tarda en ir a Okinawa en avión? [end]\n","[start] no hay nada que no hacer hoy [end] [start] Aparentemente, no hay nada que no pueda pasar hoy. [end]\n","[start] me gustan la música [end] [start] Yo disfruto escuchar música. [end]\n","[start] podría ser razón [end] [start] Puede que tengas razón. [end]\n","[start] este está está lleno [end] [start] Este está lleno. [end]\n","[start] te gusta tom [end] [start] ¿Te gusta Tom? [end]\n","[start] ellos estaban bien [end] [start] Estarán estupendamente. [end]\n","[start] la mano que [UNK] [end] [start] Dame esa escoba. [end]\n","[start] creo que eso está mal [end] [start] Creo que eso no es correcto. [end]\n","[start] tom [UNK] a mary de todo [end] [start] Tom zarandea a Mary. [end]\n","[start] están [UNK] las manos con la que está al [UNK] [end] [start] Ellos están bajando las luces. El juego está a punto de comenzar. [end]\n","[start] te vas a dormir en nada [end] [start] ¿Vas a algún lado? [end]\n","[start] en el [UNK] viene el [UNK] después de un momento [end] [start] En el alfabeto, la B va después de la A. [end]\n","[start] no sé nada de él lo que él está de él [end] [start] No sé nada de él. [end]\n","[start] te he sido de un momento que te vi [end] [start] Te amo desde el momento en que te vi. [end]\n","[start] le veo a mi madre en la cocina [end] [start] Le ayudé a mi madre en la cocina. [end]\n","[start] creo que lo que estás haciendo es feliz [end] [start] Pienso que lo que estás haciendo es peligroso. [end]\n","[start] tan pronto como le [UNK] la [UNK] que la [UNK] [end] [start] Apenas él prendió el fósforo, la bomba estalló. [end]\n","[start] deberías [UNK] tu coche [end] [start] Deberías arreglar tu coche. [end]\n","[start] tom le perdió su paraguas [end] [start] Tom perdió su paraguas. [end]\n","[start] estás enfermo o no [end] [start] ¿Eres alemán, o no? [end]\n","[start] Él tiene que tomar dos personas en la clase [end] [start] Él debe tomar dos ramos científicos. [end]\n","[start] qué te estás [UNK] [end] [start] ¿Qué estáis cocinando? [end]\n","[start] estabas solo hablando de ti cuando te vas [end] [start] Justo estábamos hablando de ti y llamaste. [end]\n","[start] casi me he pasado de todo eso [end] [start] Casi me olvidé de todo eso. [end]\n","[start] el [UNK] y la [UNK] de la [UNK] de nuestro país son [UNK] [end] [start] El aburrimiento, el acostumbramiento y la falta de curiosidad son grandes enemigos de nuestro cerebro. [end]\n","[start] no voy a hacer algo bien [end] [start] No hagamos algo de lo que nos vayamos a arrepentir. [end]\n","[start] puedo entender lo que ella está diciendo [end] [start] Entiendo lo que ella dice. [end]\n","[start] tom le [UNK] la ropa [end] [start] Tom llenó la botella de agua. [end]\n","[start] dime dónde vive [end] [start] Dime en dónde vives. [end]\n","[start] tom no sabe nada de boston [end] [start] Tom no sabe nada acerca de Boston. [end]\n","[start] esto parece francés [end] [start] Esto suena a francés. [end]\n","[start] estás [UNK] [end] [start] Usted está bajo arresto. [end]\n","[start] les dijo que no pasó lo que él no le pasó [end] [start] Te dijeron lo que pasó, ¿no? [end]\n","[start] el hombre me parecía cuando estaba en problemas [end] [start] El hombre me ayudó cuando estaba en problemas. [end]\n","[start] ella fue a la escuela a él con el dolor de cabeza [end] [start] Ella fue a la escuela a pesar de que le dolía el pie. [end]\n","[start] yo sé todo eso [end] [start] Yo sé todo eso. [end]\n","[start] Él se [UNK] de su habitación [end] [start] Él arregló su cuarto. [end]\n","[start] no le gusta nada más esa vez [end] [start] No vuelvas a hacer nada así. [end]\n","[start] lo podía [end] [start] Lo intenté. [end]\n","[start] ese trabajo no era muy interesante que la buena idea [end] [start] Ese empleo no era muy interesante. Sin embargo, pagaba bien. [end]\n","[start] espero que te vas a volver a verte [end] [start] Espero que vengas de nuevo. [end]\n","[start] tom no sabe mucho acerca de la [UNK] [end] [start] Tom no sabe mucho sobre alfarería. [end]\n","[start] los [UNK] [UNK] contra el país de [UNK] [end] [start] El senador Hoar habló duramente contra el tratado. [end]\n","[start] quién es eso [end] [start] ¿Quién es? [end]\n","[start] todavía habla francés [end] [start] ¿Sigues jugando al hockey? [end]\n","[start] todavía puedo ayudar [end] [start] Todavía puedo ayudarte. [end]\n","[start] has visto a mi teléfono de la mesa [end] [start] \"¿Has visto mi móvil?\" \"Está sobre la mesa.\" [end]\n","[start] estoy muy feliz aquí [end] [start] Estoy muy contento de estar aquí. [end]\n","[start] tuve un problema con mi coche [end] [start] Tuve un problema con mi coche. [end]\n","[start] hay más de las personas que en mi escuela [end] [start] En nuestro colegio hay más chicas que chicos. [end]\n","[start] Él [UNK] que el perro se [UNK] en la [UNK] [end] [start] Él exigió que tuvieran al perro salvaje amarrado. [end]\n","[start] creo que yo estaba demasiado [end] [start] Creo que bebí demasiado. [end]\n","[start] nunca te haré ese error [end] [start] Nunca más cometeré ese error. [end]\n","[start] tom es bueno en el [UNK] [end] [start] A Tomás se le da bien el balonmano. [end]\n","[start] tom fue el que la que [UNK] [end] [start] Tom fue quien comenzó la pelea. [end]\n","[start] eso es lo que quería hacer [end] [start] Eso es lo que quería preguntar. [end]\n","[start] tom es su madre [end] [start] Tom es mi primo. [end]\n","[start] estoy intentando [UNK] a tom de hacer eso [end] [start] Estoy intentando convencer a Tom para que lo haga. [end]\n","[start] [UNK] a el árbol de navidad [end] [start] Decoremos el árbol de Navidad. [end]\n","[0.4375, 0.2978723404255319, 0.3793103448275862, 0.22916666666666666, 0.3392857142857143, 0.2571428571428571, 0.3584905660377358, 0.43243243243243246, 0.3333333333333333, 0.46875, 0.4117647058823529, 0.41935483870967744, 0.2916666666666667, 0.3499999999999999, 0.3421052631578948, 0.3023255813953489, 0.3148148148148148, 0.1791044776119403, 0.3125, 0.3478260869565218, 0.37037037037037035, 0.26785714285714285, 0.38636363636363635, 0.3103448275862069, 0.3888888888888889, 0.36111111111111116, 0.3409090909090909, 0.3658536585365854, 0.20731707317073167, 0.45714285714285713, 0.375, 0.25925925925925924, 0.3148148148148148, 0.38095238095238093, 0.4090909090909091, 0.5, 0.3400000000000001, 0.24242424242424243, 0.2463768115942029, 0.4444444444444444, 0.26785714285714285, 0.23437499999999997, 0.3571428571428572, 0.32432432432432434, 0.43333333333333335, 0.5, 0.2702702702702703, 0.4, 0.3333333333333333, 0.38235294117647056, 0.20253164556962022, 0.34374999999999994, 0.2962962962962963, 0.41935483870967744, 0.3137254901960784, 0.31914893617021284, 0.3, 0.2295081967213115, 0.3902439024390244, 0.4594594594594595, 0.3939393939393939, 0.32, 0.36111111111111116, 0.2857142857142857, 0.3170731707317073, 0.12068965517241378, 0.26153846153846155, 0.4, 0.3023255813953489, 0.4117647058823529, 0.2708333333333333, 0.4, 0.3157894736842105, 0.42857142857142855, 0.3, 0.2238805970149254, 0.41379310344827586, 0.42857142857142855, 0.3333333333333333, 0.44, 0.22972972972972974, 0.3658536585365854, 0.3541666666666667, 0.20312499999999997, 0.5416666666666666, 0.3499999999999999, 0.43243243243243246, 0.25862068965517243, 0.34042553191489366, 0.45454545454545453, 0.33898305084745767, 0.25396825396825395, 0.42105263157894735, 0.32558139534883723, 0.2857142857142857, 0.35555555555555557, 0.3333333333333333, 0.4, 0.25, 0.38636363636363635]\n","0.3416032321416328\n"]}]}]}