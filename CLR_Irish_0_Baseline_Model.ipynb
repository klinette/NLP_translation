{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import pathlib\n","import random\n","import string\n","import re\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow.keras.layers import TextVectorization"],"metadata":{"id":"9ci9GuKxDGyO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Parsing the data\n","\n","Each line contains an English sentence and its corresponding Irish sentence.\n","The English sentence is the *source sequence* and Irish one is the *target sequence*.\n","We prepend the token `\"[start]\"` and we append the token `\"[end]\"` to the Irish sentence."],"metadata":{"id":"1wwH_2yKSxHV"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZzEUeOz1TcU7","outputId":"4d72ee5f-7b5d-4954-c6c5-6b899a883289"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["irish_text_file = pathlib.Path('/content/drive/MyDrive/en-ga.txt')\n","\n","with open(irish_text_file) as f:\n","    lines = f.read().split(\"\\n\")[:-1]\n","text_pairs = []\n","masculine_counter, feminine_counter = 0, 0\n","for line in lines:\n","    eng, gle = line.split(\"\\t\") # We use the International Organization for Standardization's code for Irish, \"gle\" \n","    gle_set = set(gle.split())\n","    # if feminine_markers.intersection(gle_set): feminine_counter += 1\n","    # if masculine_markers.intersection(gle_set): masculine_counter += 1\n","\n","    gle = \"[start] \" + gle + \" [end]\"\n","    text_pairs.append((eng, gle))\n"],"metadata":{"id":"v0y_m13LTgub"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Here's what our Irish sentence pairs look like:"],"metadata":{"id":"C6hUirTOTvZw"}},{"cell_type":"code","source":["for _ in range(5):\n","    print(random.choice(text_pairs))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x7IPUXdXTxYI","outputId":"0d13d68c-7e61-447d-95ac-e06786afccf6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["('(b) such practice would be likely to cause the average consumer to make a transactional decision that the average consumer would not otherwise make.', '[start] (b) más dóigh don chleachtas sin a bheith ina chúis leis an ngnáth-thomhaltóir do dhéanamh cinneadh idirbhirt nach ndéanfadh an gnáth-thomhaltóir murach sin. [end]')\n","(\"As regards our future relations, what we need today is more clarity on the UK's vision.\", '[start] I dtaca leis an gcaidreamh a bheidh againn amach anseo, is éard atá uainn inniu tuilleadh soiléireachta maidir le fís na Ríochta Aontaithe. [end]')\n","('[GA] (a) in the case of a first extension, not exceeding 3 months from the date of admission of the person to the centre,', '[start] [EN] (a) i gcás an chéad fhadú, nach faide ná 3 mhí ón dáta a ligeadh an duine isteach sa lárionad, [end]')\n","('Media You blocked @lsutigerzfan Are you sure you want to view these Tweets?', '[start] Meáin Chuir tú cosc ar @lsutigerzfan An bhfuil tú cinnte gur mhaith leat breathnú ar na Tweetanna seo? [end]')\n","('View the definition, word type, pronunciation, meanings, synonyms, antonym of coulterneb in italian language.', '[start] Féach ar an sainmhíniú, cineál focal, fuaimniú, bríonna, comhchiallaigh, fhrithchiallach ar coulterneb i nGaeilge. [end]')\n"]}]},{"cell_type":"markdown","source":["Now, we split the sentence pairs into training, validation, and test sets"],"metadata":{"id":"-1IlEaLUT3Df"}},{"cell_type":"code","source":["random.shuffle(text_pairs)\n","num_val_samples = int(0.15 * len(text_pairs))\n","num_train_samples = len(text_pairs) - 2 * num_val_samples\n","train_pairs = text_pairs[:num_train_samples]\n","val_pairs = text_pairs[num_train_samples : num_train_samples + num_val_samples]\n","test_pairs = text_pairs[num_train_samples + num_val_samples :]\n","\n","print(f\"{len(text_pairs)} total pairs\")\n","print(f\"{len(train_pairs)} training pairs\")\n","print(f\"{len(val_pairs)} validation pairs\")\n","print(f\"{len(test_pairs)} test pairs\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f3yubGYeT0jF","outputId":"39f3e2c9-6b13-4df4-9f36-86d94cb6cd81"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["3245618 total pairs\n","2271934 training pairs\n","486842 validation pairs\n","486842 test pairs\n"]}]},{"cell_type":"markdown","source":["## Vectorizing the text data\n","\n","We'll use two instances of the `TextVectorization` layer to vectorize the text\n","data (one for English and one for Irish),\n","that is to say, to turn the original strings into integer sequences\n","where each integer represents the index of a word in a vocabulary.\n","\n","The English layer will use the default string standardization (strip punctuation characters)\n","and splitting scheme (split on whitespace), while\n","the Irish layer will use a custom standardization, where we add the character\n","`\"¿\"` to the set of punctuation characters to be stripped.\n","\n","Note: in a production-grade machine translation model, I would not recommend\n","stripping the punctuation characters in either language. Instead, I would recommend turning\n","each punctuation character into its own token,\n","which you could achieve by providing a custom `split` function to the `TextVectorization` layer."],"metadata":{"id":"fn7Ju9OOTLmp"}},{"cell_type":"code","source":["strip_chars = string.punctuation + \"¿\" \n","strip_chars = strip_chars.replace(\"[\", \"\")\n","strip_chars = strip_chars.replace(\"]\", \"\")\n","\n","vocab_size = 15000\n","sequence_length = 20\n","batch_size = 64\n","\n","\n","def custom_standardization(input_string):\n","    lowercase = tf.strings.lower(input_string)\n","    return tf.strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")\n","\n","\n","eng_vectorization = TextVectorization(\n","    max_tokens=vocab_size, output_mode=\"int\", output_sequence_length=sequence_length,\n",")\n","gle_vectorization = TextVectorization(\n","    max_tokens=vocab_size,\n","    output_mode=\"int\",\n","    output_sequence_length=sequence_length + 1,\n","    standardize=custom_standardization,\n",")\n","train_eng_texts = [pair[0] for pair in train_pairs]\n","train_gle_texts = [pair[1] for pair in train_pairs]\n","eng_vectorization.adapt(train_eng_texts)\n","gle_vectorization.adapt(train_gle_texts)"],"metadata":{"id":"op5NTCHXTNKQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Next, we'll format our datasets.\n","\n","At each training step, the model will seek to predict target words N+1 (and beyond)\n","using the source sentence and the target words 0 to N.\n","\n","As such, the training dataset will yield a tuple `(inputs, targets)`, where:\n","\n","- `inputs` is a dictionary with the keys `encoder_inputs` and `decoder_inputs`.\n","`encoder_inputs` is the vectorized source sentence and `encoder_inputs` is the target sentence \"so far\",\n","that is to say, the words 0 to N used to predict word N+1 (and beyond) in the target sentence.\n","- `target` is the target sentence offset by one step:\n","it provides the next words in the target sentence -- what the model will try to predict."],"metadata":{"id":"2VYzYa3DTZKw"}},{"cell_type":"code","source":["def format_dataset(eng, gle):\n","    eng = eng_vectorization(eng)\n","    gle = gle_vectorization(gle)\n","    return ({\"encoder_inputs\": eng, \"decoder_inputs\": gle[:, :-1],}, gle[:, 1:])\n","\n","\n","def make_dataset(pairs):\n","    eng_texts, gle_texts = zip(*pairs)\n","    eng_texts = list(eng_texts)\n","    gle_texts = list(gle_texts)\n","    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, gle_texts))\n","    dataset = dataset.batch(batch_size)\n","    dataset = dataset.map(format_dataset)\n","    return dataset.shuffle(2048).prefetch(16).cache()\n","\n","\n","train_ds = make_dataset(train_pairs)\n","val_ds = make_dataset(val_pairs)"],"metadata":{"id":"n_8ffCvTThbE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's take a quick look at the sequence shapes\n","(we have batches of 64 pairs, and all sequences are 20 steps long):"],"metadata":{"id":"3loHRGzyTtXT"}},{"cell_type":"markdown","source":[],"metadata":{"id":"DU0J8O6AT6jS"}},{"cell_type":"code","source":["for inputs, targets in train_ds.take(1):\n","    print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')\n","    print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')\n","    print(f\"targets.shape: {targets.shape}\")"],"metadata":{"id":"csEcLEYBT7M7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Building the model\n","\n","Our sequence-to-sequence Transformer consists of a `TransformerEncoder`\n","and a `TransformerDecoder` chained together. To make the model aware of word order,\n","we also use a `PositionalEmbedding` layer.\n","\n","The source sequence will be pass to the `TransformerEncoder`,\n","which will produce a new representation of it.\n","This new representation will then be passed\n","to the `TransformerDecoder`, together with the target sequence so far (target words 0 to N).\n","The `TransformerDecoder` will then seek to predict the next words in the target sequence (N+1 and beyond).\n","\n","A key detail that makes this possible is causal masking\n","(see method `get_causal_attention_mask()` on the `TransformerDecoder`).\n","The `TransformerDecoder` sees the entire sequences at once, and thus we must make\n","sure that it only uses information from target tokens 0 to N when predicting token N+1\n","(otherwise, it could use information from the future, which would\n","result in a model that cannot be used at inference time).\n"],"metadata":{"id":"olPbNVLEUBRw"}},{"cell_type":"code","source":["class TransformerEncoder(layers.Layer):\n","    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n","        super(TransformerEncoder, self).__init__(**kwargs)\n","        self.embed_dim = embed_dim\n","        self.dense_dim = dense_dim\n","        self.num_heads = num_heads\n","        self.attention = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=embed_dim\n","        )\n","        self.dense_proj = keras.Sequential(\n","            [layers.Dense(dense_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n","        )\n","        self.layernorm_1 = layers.LayerNormalization()\n","        self.layernorm_2 = layers.LayerNormalization()\n","        self.supports_masking = True\n","\n","    def call(self, inputs, mask=None):\n","        if mask is not None:\n","            padding_mask = tf.cast(mask[:, tf.newaxis, tf.newaxis, :], dtype=\"int32\")\n","        attention_output = self.attention(\n","            query=inputs, value=inputs, key=inputs, attention_mask=padding_mask\n","        )\n","        proj_input = self.layernorm_1(inputs + attention_output)\n","        proj_output = self.dense_proj(proj_input)\n","        return self.layernorm_2(proj_input + proj_output)\n","    def get_config(self):\n","        config = super().get_config()\n","        config.update({\n","            \"embed_dim\": self.embed_dim,\n","            \"dense_dim\": self.dense_dim,\n","            \"num_heads\": self.num_heads,\n","        })\n","        return config\n","\n","\n","class PositionalEmbedding(layers.Layer):\n","    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n","        super(PositionalEmbedding, self).__init__(**kwargs)\n","        self.token_embeddings = layers.Embedding(\n","            input_dim=vocab_size, output_dim=embed_dim\n","        )\n","        self.position_embeddings = layers.Embedding(\n","            input_dim=sequence_length, output_dim=embed_dim\n","        )\n","        self.sequence_length = sequence_length\n","        self.vocab_size = vocab_size\n","        self.embed_dim = embed_dim\n","\n","    def call(self, inputs):\n","        length = tf.shape(inputs)[-1]\n","        positions = tf.range(start=0, limit=length, delta=1)\n","        embedded_tokens = self.token_embeddings(inputs)\n","        embedded_positions = self.position_embeddings(positions)\n","        return embedded_tokens + embedded_positions\n","\n","    def compute_mask(self, inputs, mask=None):\n","        return tf.math.not_equal(inputs, 0)\n","    def get_config(self):\n","        config = super().get_config()\n","        config.update({\n","            \"sequence_length\": self.sequence_length,\n","            \"vocab_size\": self.vocab_size,\n","            \"embed_dim\": self.embed_dim,\n","        })\n","        return config\n","\n","\n","class TransformerDecoder(layers.Layer):\n","    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n","        super(TransformerDecoder, self).__init__(**kwargs)\n","        self.embed_dim = embed_dim\n","        self.latent_dim = latent_dim\n","        self.num_heads = num_heads\n","        self.attention_1 = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=embed_dim\n","        )\n","        self.attention_2 = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=embed_dim\n","        )\n","        self.dense_proj = keras.Sequential(\n","            [layers.Dense(latent_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n","        )\n","        self.layernorm_1 = layers.LayerNormalization()\n","        self.layernorm_2 = layers.LayerNormalization()\n","        self.layernorm_3 = layers.LayerNormalization()\n","        self.supports_masking = True\n","\n","    def call(self, inputs, encoder_outputs, mask=None):\n","        causal_mask = self.get_causal_attention_mask(inputs)\n","        if mask is not None:\n","            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n","            padding_mask = tf.minimum(padding_mask, causal_mask)\n","\n","        attention_output_1 = self.attention_1(\n","            query=inputs, value=inputs, key=inputs, attention_mask=causal_mask\n","        )\n","        out_1 = self.layernorm_1(inputs + attention_output_1)\n","\n","        attention_output_2 = self.attention_2(\n","            query=out_1,\n","            value=encoder_outputs,\n","            key=encoder_outputs,\n","            attention_mask=padding_mask,\n","        )\n","        out_2 = self.layernorm_2(out_1 + attention_output_2)\n","\n","        proj_output = self.dense_proj(out_2)\n","        return self.layernorm_3(out_2 + proj_output)\n","\n","    def get_causal_attention_mask(self, inputs):\n","        input_shape = tf.shape(inputs)\n","        batch_size, sequence_length = input_shape[0], input_shape[1]\n","        i = tf.range(sequence_length)[:, tf.newaxis]\n","        j = tf.range(sequence_length)\n","        mask = tf.cast(i >= j, dtype=\"int32\")\n","        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n","        mult = tf.concat(\n","            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n","            axis=0,\n","        )\n","        return tf.tile(mask, mult)\n","    def get_config(self):\n","        config = super().get_config()\n","        config.update({\n","            \"embed_dim\": self.embed_dim,\n","            \"latent_dim\": self.latent_dim,\n","            \"num_heads\": self.num_heads,\n","        })\n","        return config"],"metadata":{"id":"ZXa-hgFpULRp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["embed_dim = 256\n","latent_dim = 2048\n","num_heads = 8\n","\n","encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n","x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n","encoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(x)\n","encoder = keras.Model(encoder_inputs, encoder_outputs)\n","\n","decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n","encoded_seq_inputs = keras.Input(shape=(None, embed_dim), name=\"decoder_state_inputs\")\n","x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n","x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs)\n","x = layers.Dropout(0.5)(x)\n","decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n","decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n","\n","decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n","transformer = keras.Model(\n","    [encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\"\n",")"],"metadata":{"id":"iB1VrLfjUpnF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["epochs = 5  # This should be at least 30 for convergence\n","\n","transformer.summary()\n","transformer.compile(\n","    \"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",")\n","transformer.fit(train_ds, epochs=epochs, validation_data=val_ds)"],"metadata":{"id":"KYCElnuUUy0K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["gle_vocab = gle_vectorization.get_vocabulary()\n","gle_index_lookup = dict(zip(range(len(gle_vocab)), gle_vocab))\n","max_decoded_sentence_length = 20\n","\n","\n","def decode_sequence(input_sentence):\n","    tokenized_input_sentence = eng_vectorization([input_sentence])\n","    decoded_sentence = \"[start]\"\n","    for i in range(max_decoded_sentence_length):\n","        tokenized_target_sentence = gle_vectorization([decoded_sentence])[:, :-1]\n","        predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n","\n","        sampled_token_index = np.argmax(predictions[0, i, :])\n","        sampled_token = gle_index_lookup[sampled_token_index]\n","        decoded_sentence += \" \" + sampled_token\n","\n","        if sampled_token == \"[end]\":\n","            break\n","    return decoded_sentence\n","\n","\n","test_eng_texts = [pair[0] for pair in test_pairs]\n","for _ in range(30):\n","    input_sentence = random.choice(test_eng_texts)\n","    translated = decode_sequence(input_sentence)\n","    print(input_sentence, translated)"],"metadata":{"id":"I2C4CPliU4zM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","from nltk.translate.bleu_score import sentence_bleu\n","translated_sentences = []\n","for i in range(30):\n","  input_sentence = test_eng_texts[i]\n","  translated = decode_sequence(input_sentence)\n","  translated_sentences.append(translated)\n","\n","test_sp_texts = [pair[1] for pair in test_pairs]\n","\n","bleu_scores = []\n","for j in range(len(translated_sentences)):\n","  bleu_scores.append(sentence_bleu(translated_sentences[j], test_sp_texts[j], weights = (1, 0, 0, 0)))\n","  print(translated_sentences[j], test_sp_texts[j] )\n","print(bleu_scores) \n","print(np.sum(np.array(bleu_scores))/len(bleu_scores))"],"metadata":{"id":"ca-iNAAtdNSa"},"execution_count":null,"outputs":[]}]}